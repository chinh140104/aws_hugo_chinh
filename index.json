[{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and fundamental services (Cloud Fundamentals, IAM, Budget, Support).\nWeek 2: Learning basic VPC and foundational networking concepts.\nWeek 3: Advanced EC2 inside VPC, NAT Gateway, Security Group, DNS Resolver.\nWeek 4: VPC Peering, Transit Gateway, and complex VPC-to-VPC connectivity.\nWeek 5: Compute Services: EC2, Auto Scaling, Backup, and Storage Gateway.\nWeek 6: Advanced Storage: S3, Glacier, FSx, and Storage Gateway.\nWeek 7: Advanced IAM, AWS Organizations, Identity Center, KMS.\nWeek 8: Database Services, ETL (Kinesis/Glue/Athena), DMS Migration.\nWeek 9: Workshop – Designing system architecture on AWS (Architecture Design).\nWeek 10: Workshop – Building Database + Backend + Frontend.\nWeek 11: Workshop – Completing Frontend + Deploying the entire system.\nWeek 12: Workshop – Testing, optimizing \u0026amp; writing the final project report.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Learn foundational concepts about cloud computing and AWS. Get familiar with IAM (user, group, MFA) and cost management. Practice creating an AWS account, enabling MFA, creating users, setting budgets, and working with AWS Support. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Review Module 01 theory to understand the overall concepts of cloud and AWS - Learn about AWS Global Infrastructure, Regions, AZs - Learn basic IAM and how AWS manages users/groups 08/09/2025 08/09/2025 https://youtu.be/HxYZAK1coOI?si=vi7dp01LjqDvyIF4 https://youtu.be/IK59Zdd1poE?si=n9pSqe-XCENDsR9B https://youtu.be/HSzrWGqo3ME?si=enh1H3F44IYXuziO https://youtu.be/pjr5a-HYAjI?si=NvVriAEQGRto2QPs https://youtu.be/2PQYqH_HkXw?si=eDL8iYkYSYb7Kcvk 3 - Hands-on: + Create AWS Account + Enable MFA + Create admin group \u0026amp; admin user + Validate account authentication 09/09/2025 09/09/2025 https://youtu.be/IY61YlmXQe8?si=lle8PWNXwvrGvQlN https://youtu.be/Hku7exDBURo?si=IkRmV8fhdqIEac5Z https://000001.awsstudygroup.com 4 - Learn AWS Budget \u0026amp; Cost Management - Hands-on: + Create Budget types (Cost / Usage / RI / Savings Plan) + Clean up after creating Budgets 10/09/2025 10/09/2025 https://000007.awsstudygroup.com 5 - Learn AWS Support - Review support plans - Track and manage support cases 11/09/2025 11/09/2025 https://000009.awsstudygroup.com 6 - Review the entire Module 01 - Double-check IAM users/groups - Validate MFA, budgets, and support functionality - Take notes on key points to prepare for Module 02 12/09/2025 12/09/2025 Week 1 Achievements: Overview:\nThis week I became familiar with the fundamental concepts of cloud computing and AWS, understood how AWS organizes infrastructure, manages services, and optimizes cost. I also practiced account creation, IAM setup, and Budget configuration. Theory learned:\nCloud computing concepts and AWS operating models Global infrastructure: Region, AZ, Edge Location AWS management tools (Console, CLI, SDK) Basic IAM: User, Group, Policy Cost control: Budget, Cost Explorer, Support Plans Hands-on labs:\nCreated AWS account + enabled MFA Created admin group and admin user Created and configured different Budget types Learned AWS Support plans and practiced creating support cases "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"How Uniphore Achieved 30% Cost Savings by Modernizing Windows Servers on AWS By Sornavel Perumal and Ismail Baig on June 19, 2025 in categories: Amazon EC2, Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service, Amazon Simple Storage Service (S3), AWS Backup, Customer\nUniphore is the first built-for-scale, AI-native company that brings AI to every part of the enterprise experience. Uniphore\u0026rsquo;s enterprise-grade multimodal AI and data platform unifies all elements of voice, video, text, and data using Generative AI, Knowledge AI, Emotion AI, and workflow automation together as a trusted copilot. Uniphore operates one of its core enterprise applications on legacy Windows Server infrastructure, which faced scalability and performance challenges as the business grew. Increasing maintenance costs and security concerns related to end-of-support operating systems meant Uniphore needed to modernize their infrastructure while ensuring business continuity.\nUniphore successfully utilized Amazon Web Services (AWS) services to modernize their Windows workloads through containerization and cloud-native solutions. By using Amazon Elastic Kubernetes Service (Amazon EKS) for containerization and Amazon Elastic Compute Cloud (Amazon EC2) for compute resources, combined with Amazon Elastic File System (Amazon EFS) and Amazon S3 for storage, Uniphore achieved 30% cost savings while significantly improving their operational capabilities. They migrated from Windows on-premises infrastructure to Linux on AWS to optimize call center analytics workloads. They found that their core business requirements for training and fine-tuning large language models (LLMs) performed significantly better on Linux-based systems, allowing them to build a unified, high-performance ecosystem specifically designed for AI/ML operations.\nIn this article, we explore how Uniphore approached their modernization journey and share practical insights for organizations considering similar transformations. We present a practical approach to modernizing legacy Windows workloads through containerization, efficient data migration, and cloud-native services. Whether you\u0026rsquo;re planning a similar transformation or evaluating modernization options, this real-world example shows how to overcome common migration challenges while achieving tangible business benefits like improved scalability, enhanced security, and reduced operational costs.\nBusiness Challenges Uniphore\u0026rsquo;s U-Assist platform was initially hosted on-premises on 50 physical Windows Server 2008 R2 servers. As the business rapidly expanded, these servers began experiencing significant load. Frequent storage capacity shortages led to application cache failures and operational disruptions. Data exceeding 60 TB coupled with aging hardware reaching its limits caused maintenance costs to rise while system performance declined.\nThese applications were not designed for scalability and lacked vendor support, making it increasingly difficult to meet growing business demands. Security was another significant concern, as the end-of-support status of Windows Server 2008 R2 meant no more security patches or compliance support. These challenges, combined with significant operational costs from frequent manual interventions, made it clear that modernization was necessary.\nDeployed Solution Uniphore implemented a comprehensive modernization strategy using AWS services across three main areas: 1. Application Modernization The team modernized the application stack by rewriting Java-based services to become operating system-agnostic (OS-agnostic) and implementing containerization with Docker. This included refactoring Windows-specific dependencies, such as file system paths, while separating configuration for container compatibility. The containerized application was then deployed to Amazon EKS using Kubernetes, achieving true application portability and fault tolerance across cloud environments through auto-scaling, self-healing, and rolling updates.\n2. Custom Data Migration Solution Uniphore developed a custom migration solution using Type 2 hypervisor technology. This innovative approach enabled hosting the AWS DataSync agent and seamless integration with Windows-based storage. Using high-speed dedicated internet connections, data was efficiently transferred to Amazon EFS, ensuring improved reliability and availability.\n3. Cloud Infrastructure Setup The modern infrastructure uses Amazon EC2 and Amazon EKS for compute and container orchestration, with Amazon Virtual Private Cloud (Amazon VPC) providing secure network isolation. Storage needs are met through Amazon EFS and Amazon S3, while DataSync is used to transfer data from legacy servers to Amazon EFS, as illustrated in Figure 1.\nApplications running on Amazon EKS process data retrieved from EFS, and the output is stored in Amazon S3. Processed data is backed up using AWS Backup.\nFigure 1: Uniphore\u0026rsquo;s Data Migration Architecture with DataSync and AWS Services\nTechnical Implementation To ensure a seamless and uninterrupted transition from legacy infrastructure to a modern cloud-native environment, Uniphore executed the migration in three structured phases. Each phase focused on minimizing risk, ensuring data integrity, and maintaining business continuity.\nPhase 1: Hypervisor and DataSync Agent Deployment The team began by setting up a Type 2 hypervisor (e.g., VMware Workstation or Hyper-V) on existing hardware to host the DataSync agent. This setup enabled secure connectivity between on-premises Windows Server hosts and the AWS environment.\nKey actions in this phase included:\nProvisioning and configuring the hypervisor to run the DataSync agent in an isolated, stable environment. Optimizing the network layer with high-speed dedicated internet and VPN tunnels to maximize bandwidth and reduce latency. Validating connectivity between on-premises storage and Amazon EFS targets to ensure readiness for data transfer. Phase 2: Secure Data Migration and Validation With infrastructure ready, the team began migrating over 60 TB of structured and unstructured data. DataSync was used to orchestrate data transfers reliably with close monitoring.\nTo ensure data integrity and completeness:\nTransfers were performed incrementally, with checksum validation at both ends. Transfer tasks were monitored in real-time through Amazon CloudWatch and DataSync metrics. Transfer performance was fine-tuned multiple times to optimize throughput, especially for large files and deep directory structures. A separate test environment on AWS was used to validate migrated data and application behavior before final cutover. Phase 3: Production Cutover and Application Deployment The final phase involved transitioning to production workloads hosted on AWS. The legacy Windows-based application stack was containerized using Docker and deployed to Amazon EKS, enabling flexible platform scaling and operation independent of the underlying operating system.\nKey steps in this phase:\nThe EKS cluster was configured with auto-scaling node groups using Amazon EC2, optimizing cost and performance. Amazon VPC was established with custom routing, Security Groups, and NACLs to ensure secure traffic flow between internal (east-west) and external (north-south) services. Application configuration and secrets were managed using AWS Systems Manager Parameter Store and AWS Secrets Manager. A/B testing and gradual traffic switching ensured zero-downtime deployment. Legacy systems were only decommissioned after thorough validation in the cloud environment. Best Practices The following best practices were implemented:\nInfrastructure as Code (IaC) with Terraform: Infrastructure was provisioned with Terraform, enabling version-controlled, repeatable, and auditable deployments. CI/CD pipelines: GitLab pipelines were established for continuous integration and deployment (CI/CD), ensuring fast and secure updates to containerized services. Observability: DATADOG and CloudWatch were integrated for monitoring and alerting across the entire stack, including compute, storage, and network. Backup and Disaster Recovery: AWS Backup policies were applied to Amazon EFS and Amazon EC2 snapshots, with cross-Region replication for critical workloads. Key Benefits The following six key benefits were achieved with this solution:\n1. Scalability and Performance Before modernization, Uniphore\u0026rsquo;s physical servers had fixed resources, often leading to performance bottlenecks during peak periods.\nAfter migration:\nAuto Scaling with Amazon EC2 and Amazon EKS enabled infrastructure to automatically scale up or down based on actual workload demand. This flexible scaling ensured high availability and stable performance, even during high-load periods or batch processing. Application latency was significantly reduced through optimized container orchestration and right-sized compute provisioning. 2. Flexibility and Portability Legacy applications were previously tied to a specific Windows OS version (2008 R2), making upgrades or migration difficult.\nAfter refactoring and containerization:\nApplications became OS-agnostic and independent of specific environments. Containerized services can now run on any Kubernetes-compatible platform, such as Amazon EKS, self-managed clusters, or even other cloud providers, ensuring vendor neutrality and mobility. Developers gained flexibility to test and deploy in isolated environments without dependency conflicts. 3. Enhanced Security and Compliance Operating unsupported systems like Windows Server 2008 R2 posed serious security and compliance risks, such as:\nLack of security patches. Incompatibility with modern compliance frameworks like SOC 2, HIPAA, or ISO 27001. After migration:\nUse of Amazon VPC, AWS Identity and Access Management (IAM) roles, Security Groups, and Kubernetes RBAC improved access control and network segmentation. All workloads run on actively supported operating system versions and are protected by native AWS security tools like Amazon Inspector, AWS Config, and AWS GuardDuty. AWS Backup and multi-Availability Zone (AZ) architecture provide built-in disaster recovery and business continuity. 4. Efficient Data Management Data growth velocity had exceeded on-premises infrastructure capabilities, leading to storage capacity issues and backup failures.\nAmazon EFS provides a fully managed NFS file system that automatically scales with data growth. Amazon S3 is used to store processed data, benefiting from high durability and lifecycle policies to move older data to cost-effective storage tiers like S3 Glacier. DataSync enabled fast and secure transfer of over 60 TB of data while ensuring integrity with checksum and retry mechanisms. 5. Reduced Operational Costs Managing on-premises infrastructure incurred high operational costs, from manual patching and monitoring to capacity planning.\nAfter migration:\nThe team no longer worries about hardware maintenance, OS upgrades, or backup scripting. IaC with Terraform and automated CI/CD pipelines enabled more consistent, reliable infrastructure management and deployment. The engineering team can now focus on product innovation instead of infrastructure troubleshooting. 6. Significant Cost Savings One of the most impactful outcomes of this initiative was a 30% reduction in total operational costs.\nCost savings were achieved through:\nDecommissioning legacy data center resources. Optimizing instance usage with EC2 Auto Scaling and Spot Instances. Right-sizing storage and compute resources. Containerization reducing resource costs, allowing multiple applications to run efficiently on fewer nodes. Ability to scale down unused resources during off-peak hours directly reducing monthly cloud bills. Conclusion This article illustrates how enterprises can successfully modernize legacy Windows Server workloads using AWS services, through Uniphore\u0026rsquo;s journey—from physical servers to containerized, cloud-native architecture.\nThrough careful planning and implementation of containerization, custom data migration solutions, and modern cloud infrastructure, organizations can achieve significant operational and cost benefits while maintaining business continuity.\nKey takeaways from this modernization journey include:\nThe importance of a phased migration approach to reduce risk and ensure data integrity. How containerization can transform legacy Windows applications into flexible, scalable workloads. Effective data migration strategies using AWS DataSync and custom solutions. Best practices for deploying security, monitoring, and disaster recovery in the cloud environment. Learn More To begin your own modernization journey, please see the following resources:\nhttps://aws.amazon.com/enterprise/modernization/ https://www.youtube.com/watch?v=iL5FJ_1vKik https://www.youtube.com/watch?v=4rquMWa3qp0 TAGS: Amazon EKS, Amazon Elastic Compute Cloud (Amazon EC2), Amazon Simple Storage Service (Amazon S3), AWS Cloud Storage, AWS DataSync\nSornavel Perumal Sornavel Perumal is a Senior Technical Account Manager at AWS with over 20 years of experience. He helps customers build scalable and cost-optimized solutions with AWS. He is passionate about Analytics, Machine Learning, and GenAI, and always enjoys learning new things every day.\nIsmail Baig Ismail Baig is the Manager of DevOps and Cloud at Uniphore and an AWS Certified Solutions Architect professional. With over 12 years of experience, he specializes in solving complex infrastructure challenges for next-generation startups in Analytics, AI, ML, Big Data, and Digital Solutions.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Updates to the AWS MSSP Competency: Delivering Turnkey Security Solutions for Customers By Brian Mendenhall, Joanne Moore, and Aliaksei Ivanou on June 17, 2025 in categories: Announcements, AWS re:Inforce, Foundational (100), Launch, Partner solutions, Security Permalink Comments Share\nBy Brian Mendenhall, Global Head, Security \u0026amp; Identity Partner Specialist — AWS\nBy Aliaksei Ivanou, Global Security \u0026amp; Identity Partner Solutions Architect — AWS\nBy Joanne Moore, Senior Implementation Manager, AWS Specialization Program — AWS\nIn today\u0026rsquo;s digital landscape, businesses face increasingly complex security challenges: managing multiple security tools, handling vendor relationships, and maintaining internal security expertise—all while still operating their core business functions.\nManaged Security Service Providers (MSSPs) help address these challenges by providing comprehensive security solutions that transform how organizations approach cloud security. As a single point of contact, they coordinate multiple security capabilities, from native AWS services to industry-leading security tools.\nTo support these evolving needs, we\u0026rsquo;re excited to introduce new categories for the AWS MSSP Competency (formerly AWS Level 1 MSSP Competency), designed to help customers easily identify Partners with security expertise based on their specific requirements. The new MSSP Competency categories validate the capability of Service Partners to deliver turnkey outcomes incorporating native AWS services, and the ability of Software Partners to integrate with native AWS services.\nIn addition to core MSSP requirements, each AWS MSSP Competency Partner must select and meet requirements for at least one category. This allows Partners to demonstrate deep technical expertise in specific security domains, differentiate themselves in the marketplace, and better connect with customers seeking their particular security strengths.\nAdditionally, MSSP Competency Partners have the option to showcase how they integrate validated ISV solutions from AWS Security Competency Partners into their managed security service. This level of visibility helps AWS customers identify which MSSP Competency Partners can effectively manage their existing third-party security tools as part of a comprehensive security solution.\nAWS MSSP Competency Partners are validated for their ability to provide end-to-end security management, maintaining strong security posture through continuous monitoring, advanced threat detection, and incident response. These partners ensure robust security while enabling businesses to focus on core operations.\nNew! AWS MSSP Competency Categories The AWS MSSP Competency categories help customers identify validated Partners with expertise who can deliver comprehensive security outcomes with quality, speed, and cost efficiency.\nInfrastructure Security Focuses on managing the security posture of foundational infrastructure components through leading controls for vulnerability management, configuration security, and patch management. Includes network layer protection through proactive Distributed Denial of Service (DDoS) protection, network threat prevention, and network access control, along with deploying and managing network segmentation strategy including segment isolation, micro-segmentation, and network policy management.\nWorkload Security Provides turnkey security at the workload level, specifically for containers or hosts. Services are validated for their ability to provide cloud infrastructure vulnerability management for workloads, host-based security, container security, and serverless security.\nApplication Security Provides operational security for the application layer within AWS environments through application vulnerability management, API security, managed Web Application Firewall, and pipeline security.\nData Protection Helps protect customer data through comprehensive data security programs to prevent data loss, deploy and manage encryption strategy, discover sensitive data in their environments, and manage data access permissions.\nIdentity \u0026amp; Access Management Provides turnkey Identity \u0026amp; Access Management (IAM) solutions through guided identity governance, aligned with customer business needs. Partners deploy and manage authentication controls aligned with that governance, manage and monitor privileged accounts, and secure temporary and just-in-time elevated access.\nIncident Response Creates incident response preparation plans, identifies key stakeholders in case of incidents, and provides tabletop exercises for preparation. Includes incident investigation, classification, and remediation, along with post-incident analysis and forensic investigation.\nCyber Recovery Operationalizes rapid recovery from security events through cyber business continuity planning, backup management programs, ransomware protection, and cyber disaster recovery planning and execution.\nCustomers: Working with AWS MSSP Partners AWS MSSP Competency Partners provide turnkey security for your AWS environment, using native AWS services along with third-party ISV tools you may already be using.\nExplore validated AWS MSSP Competency Partner solutions by category.\nPartners: Become an AWS MSSP Competency Partner\nThe AWS MSSP Competency is available to both Service and Software Partners with solutions that help deliver turnkey security outcomes within customer AWS environments.\nIn addition to the benefits of the AWS Specialization Program, MSSP Competency Partners receive distinct benefits, including exclusive market access opportunities through AWS-led security marketing campaigns, private briefings, and participation in Security LIVE! events.\nTo qualify, Partners must be a validated member of the Software Path and complete a Foundational Technical Review (FTR) for their MSSP solution, or achieve Advanced Tier or higher in the Services Path.\nTo apply, review the Program Guide and access the application in AWS Partner Central (login required).\nTAGS: AWS Competency Partners, Level 1 Managed Security Services, MSSP, re:inforce\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Unlocking Retail Intelligence by Transforming Data into Actionable Insights Using Generative AI with Amazon Q Business By Suprakash Dutta, Abhijit Dutta, Girish Nazhiyath, Krishnan Hariharan, Alberto Alonso and Ramesh Venkataraman — July 09, 2025 — in Amazon Q Business, Amazon QuickSight, Analytics, Artificial Intelligence, Generative AI, Intermediate (200), Retail, Technical How-to Permalink Comments Share\nBusinesses often face challenges in managing and extracting value from their data. According to McKinsey, 78% of organizations currently use AI in at least one business function (as of 2024), showing the growing importance of AI solutions in business. Additionally, 21% of organizations using generative AI have completely redesigned their workflows, demonstrating how AI is transforming business operations.\nGartner identifies AI-powered analytics and reporting as a core investment area for retail organizations, with most major retailers expected to deploy or scale such solutions within the next 12-18 months. The complexity of data in the retail industry requires sophisticated solutions that can seamlessly integrate with existing systems. Amazon Q Business provides features that can be customized to meet specific business needs, including integration capabilities with popular retail management systems, point-of-sale (POS) systems, inventory management software, and e-commerce systems. Through advanced AI algorithms, the system analyzes historical data and current trends, helping businesses effectively prepare for seasonal fluctuations in demand and make data-driven decisions.\nAmazon Q Business for Retail Intelligence is an AI-powered assistant designed to help retail businesses streamline operations, improve customer service, and enhance decision-making processes. This solution is specifically designed to be scalable and adaptable to businesses of various sizes, helping them compete more effectively. In this article, we will demonstrate how you can use Amazon Q Business for Retail Intelligence to transform your data into actionable insights.\nSolution Overview Amazon Q Business for Retail Intelligence is a comprehensive solution that transforms how retailers interact with their data using generative AI. The solution architecture combines the powerful generative AI capabilities of Amazon Q Business and visualizations from Amazon QuickSight to deliver actionable insights across the entire retail value chain. Our solution also uses Amazon Q Apps so retail roles and users can create custom AI-powered applications to streamline daily tasks and automate workflows and business processes.\nThe following diagram illustrates the solution architecture.\nThe solution uses the AWS architecture above to deliver a secure, high-performance, and reliable solution for retail intelligence. Amazon Q Business serves as the primary generative AI engine, enabling natural language interaction and powering custom retail-specific applications. This architecture includes AWS IAM Identity Center for robust authentication and access control, and Amazon Simple Storage Service (Amazon S3) providing secure data lake storage for retail data sources. We use QuickSight to create interactive visualizations, enhancing data interpretability. The solution\u0026rsquo;s flexibility is further enhanced by AWS Lambda for serverless processing, Amazon API Gateway for efficient endpoint management, and Amazon CloudFront for optimized content delivery. This solution uses custom Amazon Q Business plugins to call API endpoints to initiate automation processes directly from the Amazon Q Business web application interface based on customer queries and interactions.\nThis configuration deploys a three-tier architecture: Data integration layer: securely ingests data from multiple retail sources, Processing layer: where Amazon Q Business analyzes queries and generates insights, Presentation layer: delivers personalized, role-based insights through a unified interface.\nWe have provided a sample AWS CloudFormation template, sample dataset, and scripts that you can use to set up the environment for this demo.\nIn the following sections, we will dive into how this solution works.\nImplementation We have provided the Amazon Q Business for Retail Intelligence solution as open source—you can use it as a starting point for your own solution and help us improve by contributing bug fixes and features through pull requests on GitHub.\nVisit the GitHub repository to explore the code, select Watch to receive notifications about new releases, and check the README file for the latest documentation updates.\nOnce you set up the environment, you can access the Amazon Q Business for Retail Intelligence dashboard, as illustrated in the following screenshot.\nYou can interact with QuickSight visualizations and the Amazon Q Business chat interface to ask questions using natural language.\nKey Features and Capabilities Retail users can interact with this solution in various ways. In this section, we will explore the key features.\nFor senior executives or business leaders who want to know how your business is performing, our solution provides a unified interface, making it easy to access and interact with both qualitative and quantitative business data through natural language. For example: users can analyze quantitative data like product sales or marketing campaign effectiveness using interactive visualizations from QuickSight, and qualitative data like customer feedback from Amazon Q Business, all within a single interface.\nSuppose you are a marketing analyst and you want to evaluate campaign effectiveness and reach across channels, while conducting advertising spend versus revenue analysis. With Amazon Q Business, you can run complex queries using natural language questions and share Q Apps with multiple teams. This solution provides automated insights into customer behavior and campaign performance, helping marketing teams make faster decisions and adjust flexibly to maximize ROI.\nSimilarly, suppose you are a merchandiser or supplier manager, and you want to understand the impact of high-cost events on your international business—where you need to handle import and export of goods and services. You can input information into Amazon Q Apps and receive feedback based on that specific product or product group.\nUsers can also submit requests via API using custom Amazon Q Business plugins for real-time interaction with backend applications. For example, a store manager might want to know which items in current inventory need to be replenished or rebalanced for the upcoming week based on weather forecasts or local sports events.\nTo learn more, refer to the complete demo below.\nYour browser does not support the video tag. In this article, we do not use Amazon Q\u0026rsquo;s generative business intelligence (BI) capability combined with QuickSight visualizations. To learn more, see Amazon Q in QuickSight.\nEmpowering Retail Roles with AI-Driven Intelligence Amazon Q Business for Retail Intelligence changes how retailers handle their data challenges through a generative AI-powered assistant. This solution seamlessly integrates with existing systems, using Retrieval Augmented Generation (RAG) to unify disparate data sources and provide actionable insights in real-time. Below are some key benefits for different roles:\nSenior Executives — Access comprehensive real-time dashboards for enterprise-wide metrics and KPIs, while using AI-powered recommendations for strategic decisions. Use predictive analytics to forecast consumption trends and enable proactive strategy adjustments for business growth.\nMerchandise Managers — Gain immediate insights into sales trends, profit margins, and inventory turnover rates through automated analytics tools and AI-powered pricing strategies. Identify and leverage emerging trends through predictive analytics to optimize product assortment and structure.\nInventory Managers — Perform data-driven inventory level optimization across multiple store locations while streamlining operations with automatic reorder point calculations. Accurately predict and prepare for seasonal fluctuations to maintain optimal inventory levels during peak periods.\nStore Managers — Maximize operational efficiency through AI-predictive workforce optimization, while accessing insights about local conditions affecting store performance. Compare store metrics with other locations using sophisticated benchmarking tools to identify improvement opportunities.\nMarketing Analysts — Monitor and analyze marketing campaign effectiveness across channels in real-time while developing sophisticated customer segments using AI-powered analytics. Calculate and optimize marketing ROI across channels for effective budget allocation and improved campaign performance.\nAmazon Q Business for Retail Intelligence makes complex data analysis more accessible to various types of users through natural language interface. This solution enables data-driven decision-making across the organization by providing role-based insights, breaking down traditional data silos. By providing each retail role with appropriate analytics and action recommendations, organizations can achieve higher operational efficiency and maintain competitive advantage in today\u0026rsquo;s dynamic retail landscape.\nConclusion Amazon Q Business for Retail Intelligence combines generative AI capabilities with powerful visualization tools to revolutionize retail operations. By enabling natural language interaction with complex data systems, this solution democratizes data access across all organizational levels—from senior executives to store managers. The system\u0026rsquo;s ability to provide role-based insights, automate workflows, and support real-time decision-making makes it a crucial tool for retail businesses seeking to maintain competitiveness in today\u0026rsquo;s dynamic environment. As retailers continue to adopt AI-driven solutions, Amazon Q Business for Retail Intelligence can help meet the industry\u0026rsquo;s growing demand for sophisticated data analysis and operational efficiency.\nTo learn more about our solutions and services, please refer to Amazon Q Business and Generative AI on AWS. For expert assistance, AWS Professional Services, AWS Generative AI partner solutions, and AWS Generative AI Competency Partners are always ready to help.\nAbout the Authors Suprakash Dutta is a Senior Solutions Architect at Amazon Web Services, leading strategic cloud transformation programs for Fortune 500 retailers and large enterprises. He specializes in designing mission-critical cloud-based retail solutions, implementing generative AI, and retail modernization initiatives. He is a multi-certified cloud architect and has delivered transformation solutions that modernize operations across thousands of retail stores while achieving superior efficiency through AI-powered retail intelligence solutions.\nAlberto Alonso is a Specialist Solutions Architect at Amazon Web Services. He focuses on generative AI and how it can be applied to business challenges.\nAbhijit Dutta is a Sr. Solutions Architect in the Retail/CPG segment at AWS, focusing on key areas such as legacy application migration and modernization, data-driven decision making, and AI/ML capability deployment. His expertise lies in helping organizations leverage cloud technology for digital transformation initiatives, with particular emphasis on analytics and generative AI solutions.\nRamesh Venkataraman is a Solutions Architect who enjoys working with customers to solve technical challenges using AWS services. Outside of work, Ramesh enjoys following questions on Stack Overflow and answering whenever possible.\nGirish Nazhiyath is a Sr. Solutions Architect in the Retail/CPG segment of Amazon Web Services. He enjoys working with retail/CPG customers to drive technology-based retail innovation, with over 20 years of experience across multiple retail domains and segments globally.\nKrishnan Hariharan is a Sr. Manager, Solutions Architecture at AWS, based in Chicago. In his current role, he uses a diverse combination of customer, product, technology, and operational skills to help retail/CPG customers build optimal solutions with AWS. Before joining AWS, Krishnan was President/CEO at Kespry and COO at LightGuide. He holds an MBA from The Fuqua School of Business, Duke University, and a Bachelor of Science in Electronics from Delhi University.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.8-cognito/5.8.1-create-user-pool/","title":"Create Cognito User Pool","tags":[],"description":"","content":"Create User Pool In this step, you will create a Cognito User Pool to manage user authentication for your application.\nNavigate to AWS Cognito service in the AWS Console Click Create user pool Configure options\nOptions for sign-in identifiers:\nSelect Email (allows users to sign in with email) Uncheck Phone number and Username Self-registration:\n✅ Enable self-registration (allows users to sign up themselves) Required attributes for sign-up:\nSelect email and name as required attributes Click Create user pool Enter User Pool Name\nUser pool name: TaskManagementUserPool Click Create user pool Verify User Pool Creation After successful creation, you should see your User Pool in the Cognito console with default configurations:\nReview Default Configuration Cognito automatically creates the User Pool with default settings:\nSign-in experience:\nSign-in options: Username (can be changed to Email) Password policy: Cognito defaults Multi-factor authentication: Optional Sign-up experience:\nSelf-service sign-up: Enabled Email verification: Enabled Required attributes: email Message delivery:\nEmail provider: Send email with Cognito App integration:\nHosted UI: Not configured (will be configured in later steps) Important Note ⚠️ Warning: Options for sign-in identifiers and required attributes cannot be changed after the User Pool is created. If you need to change these, you must create a new User Pool.\nNote down the User Pool ID from the overview page as you\u0026rsquo;ll need it for the next steps:\nIn the following steps, we will configure detailed features like password policies, email verification, and create App Clients.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Huỳnh Trung Chính\nPhone Number: 0975871401\nEmail: huynhtrunchinh10a1@gmail.com\nUniversity: Ho Chi Minh City University of FPT\nMajor: Software engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Building the TaskHub Platform with the DevSecOps Model on AWS Workshop Introduction In this workshop, you will build the entire TaskHub platform following the AWS Serverless + DevSecOps model, based on real-world architectures widely adopted by modern enterprises to achieve scalability, security, and cost efficiency.\nThe workshop is divided into groups of AWS services, helping you:\nUnderstand the role of each service in a modern serverless architecture. Deploy API Gateway, Lambda, DynamoDB, Cognito, and S3/CloudFront hands-on. Apply DevSecOps practices using CodePipeline, CodeBuild, and CodeGuru. Implement security at both the Backend (KMS, Secrets Manager) and Edge (WAF, Shield). Fully integrate a Next.js frontend with an AWS serverless backend. Architectural Overview In this workshop, you will build all the key components of the TaskHub platform:\nAmazon S3 – Stores the static build of the Next.js application. Amazon CloudFront – Delivers the UI globally with low latency. AWS WAF \u0026amp; AWS Shield – Protect the application from DDoS attacks and OWASP Top 10 threats. Amazon Cognito – Handles user authentication, identity management, and Admin/Member role authorization. Amazon API Gateway – Serves as the entry point for frontend requests. AWS Lambda (Node.js/TypeScript) – Processes all business logic. Amazon DynamoDB – Stores tasks, users, and progress data. AWS KMS – Encrypts data stored in DynamoDB. AWS Secrets Manager – Stores sensitive information and API keys securely. AWS CodePipeline – Automates the entire CI/CD process. AWS CodeBuild – Builds frontend/backend and performs security scans. AWS CodeGuru Reviewer – Analyzes code quality and provides optimization recommendations. AWS CloudFormation – Deploys infrastructure via IaC. Amazon CloudWatch Logs – Captures logs from Lambda and API Gateway. AWS X-Ray – Provides system-wide tracing and latency analysis. Amazon SNS – Sends system events and alert notifications. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Reflection Report: “Kick-off AWS First Cloud Journey Workforce OJT Fall 2025” Purpose of the Event Officially launch the On-the-Job Training (OJT) journey for FPTU students in the AWS First Cloud Journey program. Provide career inspiration and direction in Cloud, DevOps, and AI/ML fields through insights from industry experts and successful alumni. Facilitate direct networking opportunities between students, the university, businesses, and the AWS community. List of Key Speakers \u0026amp; Contributors Mr. Nguyễn Trần Phước Bảo – Head of Corporate Relations, FPT University (Opening Remarks) Mr. Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Mr. Đỗ Huy Thắng – DevOps Lead, VNG Mr. Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova (FCJ Alumni) Ms. Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne (FCJ Alumni) Mr. Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific (FCJ Alumni) Mr. Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific (FCJ Alumni) Highlights \u0026amp; Key Learnings 1. Vision from AWS \u0026amp; Career Direction AWS First Cloud Journey: The program aims not only to train skills but to cultivate a generation of AWS Builders for Vietnam, ready for the digital transformation wave. Clear Learning Path: The OJT program provides hands-on training from cloud fundamentals (AWS Cloud Practitioner) to specialized areas (Solutions Architect, DevOps, AI/ML). Strong Community: Students are connected to the AWS Study Group with over 47,000 members – an invaluable resource for learning and support. 2. Practical Insights from Industry: What is DevOps? DevOps is More Than Tools: Mr. Đỗ Huy Thắng (VNG) emphasized that DevOps is about culture and process that bridges Development and Operations, aiming for faster and more stable product releases. Essential Skills: Beyond AWS knowledge, Git, and CI/CD; communication, problem-solving, and self-learning abilities are critical for career growth. 3. Inspiring Journeys from Alumni From FCJ to GenAI Engineer: Mr. Hiếu Nghị shared his journey from being an FCJ student, continuous self-learning, to seizing opportunities in Generative AI – demonstrating career evolution in tech. She in Tech: Ms. Linh Nhi inspired attendees about overcoming stereotypes and confidently pursuing technical passions, highlighting diversity and equal opportunity in the industry. A Day in the Life of a Cloud Engineer: Mr. Hải Anh described real daily tasks: reviewing architecture, writing Infrastructure as Code (Terraform), troubleshooting, and cost optimization, helping students visualize their future roles. Lesson on Proactivity: Mr. Thanh Hiệp stressed that opportunities don\u0026rsquo;t come automatically. Proactively seeking opportunities (like applying to FCJ), persistent learning, and connecting with mentors were key to his success. Key Takeaways For Personal Direction Define Clear Goals: The FCJ program is a launchpad, but each individual needs to chart their own path (toward Solutions Architect, DevOps Engineer, AI Engineer, etc.). Importance of Mindset: A growth mindset, readiness for challenges, and continuous learning are more important than knowing a specific tool. For Professional Skills Soft Skills are Crucial: Communication, teamwork, and presentation skills were mentioned by all speakers as indispensable. Theory Must Meet Practice: Classroom knowledge is foundational; real value comes from building actual projects, participating in competitions, and gaining internship experience. For Opportunities \u0026amp; Networking Value of Networking: The event was a golden opportunity to connect with seniors who could become mentors or referrals in the future. Community Support: Learning to leverage the AWS Study Group and official program support channels is essential to avoid feeling \u0026ldquo;lost\u0026rdquo; during the learning journey. Personal Action Plan After the Event Create a Detailed Study Plan: Based on the OJT roadmap, develop a personal schedule to obtain the AWS Cloud Practitioner certification within the next 2 months. Proactive Networking: Connect on LinkedIn with the speakers and alumni who shared their stories, and actively participate in discussions within the AWS Study Group. Build a Portfolio: Start a small personal project (e.g., deploying a static website to AWS S3) to immediately apply learned knowledge and enhance my CV. Develop Soft Skills: Register for workshops on presentation and teamwork skills organized by the University\u0026rsquo;s Youth Union. Personal Experience at the Event Attending the Kick-off event on the 26th floor of Bitexco Tower was a memorable and inspiring experience. The professional atmosphere, from registration and check-in to the speakers\u0026rsquo; presentations, made me feel like I was truly stepping into an industry environment.\nThe most profound impression came from the alumni sharing sessions. Listening to Mr. Hiếu Nghị recount his journey from an FCJ student to a GenAI Engineer, and Ms. Linh Nhi share the challenges and successes of women in Tech, made me realize that the path ahead is entirely achievable with enough determination. These stories were far more relatable and tangible than theoretical lectures.\nThe Q\u0026amp;A session was also highly valuable, as my peers and I received direct answers to questions about the curriculum, internship opportunities, and necessary skills. The group photo with all OJT students and the organizers concluded a meaningful morning, leaving a feeling of excitement and readiness for the upcoming journey.\nIn summary, the event not only provided information but also ignited passion and confidence within me. I feel grateful for this opportunity and am committed to maximizing the AWS First Cloud Journey program for my personal development.\nSome photos from the event (Photo of check-in at the event lobby with AWS and FPTU backdrop) (Panoramic photo of the auditorium with students listening to the speaker) (Group commemorative photo with speakers and organizers after the event concluded) "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/2-proposal/","title":"Proposal","tags":[],"description":"","content":"TaskHub - Task and Progress Management Platform following the DevSecOps Model on AWS 1. Executive Summary TaskHub is a task and progress management platform designed to help working groups or small to medium-sized businesses manage work, deadlines, and progress in a visual and secure manner.\nThe system is developed following the DevSecOps model, built entirely on AWS Serverless, ensuring scalability, security, and cost optimization.\nThe development and deployment process uses AWS CodePipeline and CodeBuild to automate CI/CD and security testing.\n2. Problem Statement Problem:\nSmall businesses and project teams often struggle with managing workload, tracking progress, and distributing tasks among members. Popular management tools like Jira or Asana often have high costs and lack seamless support for DevSecOps processes or the AWS environment. Solution:\nTaskHub uses a Serverless architecture on AWS to build a lightweight, secure, and cost-effective platform. The platform is developed using AWS Lambda, API Gateway, DynamoDB, Cognito, and S3/CloudFront, while integrating AWS CodePipeline for CI/CD and dynamic security testing. Benefits and Return on Investment (ROI)\nThe TaskHub solution brings many practical benefits for development teams and small to medium-sized businesses. The system serves as a central platform for managing tasks, tracking progress, and delegating authority to members effectively. The application of the serverless model on AWS helps minimize operating costs, optimize resources, and increase scalability as usage demand grows. Furthermore, this platform supports building a practical DevSecOps environment, paving the way for research and development teams to expand further projects. According to estimates from the AWS Pricing Calculator, the system\u0026rsquo;s operating cost is only about $0.66 per month, equivalent to $7.92 per year, while the entire initial infrastructure leverages shared services from AWS, with no additional hardware costs. The expected payback period is achieved within 6-12 months due to significantly reduced manual management work and optimized internal workflow.\n3. Solution Architecture The TaskHub platform is built based on AWS Serverless architecture, ensuring operational scalability, high performance, and cost-effective operation. The system focuses on task management, teamwork, and real-time project progress, while maintaining an automated development and deployment process through the DevSecOps model.\nThe overall architecture includes key components such as Amazon API Gateway taking responsibility for receiving and distributing user requests, AWS Lambda handling business logic backend and interacting with the Amazon DynamoDB database to store task information, users, and access permissions.\nThe web interface is hosted via Amazon S3 and distributed globally by Amazon CloudFront, while AWS Cognito ensures authentication and user authorization.\nThe CI/CD process is automated using AWS CodePipeline combined with AWS CodeBuild, enabling continuous development deployment and security testing without server management.\nThe entire architecture is protected by AWS WAF and AWS KMS to enhance data security and ensure additional DevSecOps compliance. AWS X-Ray is used to monitor performance and analyze latency. The overall architecture is described in detail in the diagram below:\nAWS Services Used Amazon Route 53: Highly reliable DNS service, routing traffic. AWS WAF (Web Application Firewall): Advanced protection layer, blocking common attacks. Amazon CloudFront: Global distribution of user interface and static content. Amazon S3 (Simple Storage Service): Static hosting of the entire web interface source code (Next.js build files). Amazon Cognito: User authentication and authorization management. Amazon API Gateway: Middleware communication layer, performing authentication and routing API requests to Lambda. AWS Lambda: Core business logic processing. Integrated to log activities into CloudWatch Logs. Amazon DynamoDB: High-performance NoSQL database. Data is encrypted using AWS KMS. AWS SNS (Simple Notification Service): Handles asynchronous notifications. AWS Secrets Manager: Secure storage, management, and rotation of secrets. AWS CodePipeline, CodeBuild, \u0026amp; CodeGuru: CodePipeline/CodeBuild: Building and automating the CI/CD process. CodeBuild runs automated tests and Static Application Security Testing (SAST). AWS CodeGuru: Automated source code analysis tool, integrated into the CI/CD process to provide intelligent recommendations for performance optimization and code quality improvement, especially important in the Lambda environment. AWS CloudFormation: Infrastructure as Code (IaC) service for deploying all resources. AWS CloudWatch Logs \u0026amp; AWS X-Ray: CloudWatch Logs collects logs. CloudWatch uses this data to set up alerts. AWS X-Ray provides in-depth request tracing capabilities. Component Design User Interface Layer (Frontend):\nInterface: Next.js application built as static files. Hosting \u0026amp; Distribution: Static files are securely stored in Amazon S3 (configured as Origin for CloudFront). This interface is distributed globally by Amazon CloudFront with low latency, while being protected by AWS WAF (Web Application Firewall) at the Edge layer. Business Logic Layer (Backend):\nAPI Gateway: Amazon API Gateway receives all requests. It is configured with Cognito Authorizer to validate user tokens before forwarding requests. Processing: Lambda Functions are responsible for handling business logic (CRUD tasks, team management, permissions). Secret Management: Each Lambda function accesses sensitive information (such as external API keys) through AWS Secrets Manager, ensuring secrets are never hard-coded. Data Layer (Database):\nDatabase: Amazon DynamoDB is used to store task data, progress, and user configuration. DynamoDB operates in On-Demand mode for automatic scaling and cost optimization. Data Security: All data at rest in DynamoDB is encrypted using keys managed by AWS KMS (Key Management Service), meeting the highest security standards. Security and Authentication:\nAuthentication: Amazon Cognito provides login mechanisms, session management, and Role-Based Access Control (RBAC) for users. Cognito also supports Multi-Factor Authentication (MFA) and SSO (Single Sign-On) integration. Edge Protection: AWS WAF is placed in front of CloudFront to prevent Layer 7 DDoS attacks and other common security vulnerabilities (OWASP Top 10). Deployment and Monitoring:\nCI/CD DevSecOps: Source code is stored on GitLab (as per the diagram) and automated via the AWS CodePipeline/CodeBuild chain. This process includes running CodeGuru to optimize code before deploying infrastructure via CloudFormation. Monitoring \u0026amp; Debugging: AWS CloudWatch Logs collects detailed logs from all services. AWS CloudWatch uses these logs to set up automatic alerts for errors or incidents. AWS X-Ray provides an overview of transaction flow performance, aiding in debugging and latency optimization. 4. Technical Implementation The development of the TaskHub project is divided into two main parts—building the AWS serverless infrastructure and developing the task management platform—each including the following key implementation phases:\nDeclared Development Phases\nPhase 1: Design and Modeling (Month 1)\nMain Action: Research Serverless/DevSecOps, select core services (Lambda, DynamoDB, API Gateway). Design detailed architecture diagrams and NoSQL data models. Deliverables: Architecture Diagram and Data Model Documentation. Phase 2: Infrastructure as Code Initialization (Month 2)\nMain Action: Calculate detailed operating costs. Use AWS CDK to build IaC source code for platform services (S3, CloudFront, Cognito), ensuring environment reproducibility. Deliverables: Base AWS CDK source code and Operating Cost Report. Phase 3: DevSecOps Automation Setup (Month 2-3)\nMain Action: Set up a complete CI/CD Pipeline (CodePipeline/CodeBuild). Integrate AWS CodeGuru and SAST tools to automate source code quality and security checks before deployment. Deliverables: Operational CodePipeline and automated security scanning process. Phase 4: Development and Deployment (Month 3-4)\nMain Action: Develop functionality (Lambda Functions with TypeScript) and interface (Next.js). Perform Integration Testing between services. Deploy the official production release via Pipeline. Deliverables: TaskHub Beta Version (complete CRUD) and Testing Report. Technical Requirements\nArchitecture and Tools: The entire system is declared and managed using AWS CDK to ensure infrastructure consistency. Technology: Backend uses TypeScript/Node.js. Frontend uses Next.js (React). Source Code Management: Source code on GitLab, automated deployment via AWS CodePipeline. Monitoring: Configure CloudWatch, X-Ray, and CloudWatch Logs for performance monitoring and in-depth debugging. Non-functional Requirements: The system is located in Singapore (ap-southeast-1) to optimize speed in Vietnam, capable of scaling up to 50 users, and uses AWS KMS for data encryption. 5. Timeline \u0026amp; Key Milestones Project Timeline Pre-internship (Month 0): Prepare plans, research DevSecOps and AWS Serverless services. Month 1: Set up development environment, initiate AWS infrastructure and CI/CD pipeline. Month 2: Design architecture, develop core functionality, and automate security testing. Month 3: Integrate frontend-backend, develop testing, and launch the platform. Post-launch: Maintenance, performance evaluation, and expansion of advanced features. 6. Budget Estimate Resource Responsibility Rate (USD) / Hour Solution Architects [1 person] System Architecture Design, API design, Database Schema, Technical Leadership 6 Engineers [3 person] Backend Development, Frontend Development, Security Implement 4 Other (DevOps) [1 person] CI/CD, Cloud Deployment, Monitoring, Security, Security Configuration 4 Project Phase Solution Architects Engineers Other (Please specify) Total Hours System Design \u0026amp; Architecture 20 10 0 30 Backend Development 10 80 0 90 Frontend Development 5 60 0 65 Security \u0026amp; CI/CD Setup 5 30 10 45 Testing \u0026amp; Deployment 5 30 0 35 Total Hours 45 210 10 265 Total Cost (USD) 270 840 40 800 Cost Contribution distribution between Partner, Customer, AWS:\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 800 100 7. Risk Assessment Risk Matrix\nNetwork or AWS service disruption: Medium impact, medium probability. CI/CD deployment errors: High impact, low probability. Exceeding AWS budget: Medium impact, low probability. Security vulnerabilities: High impact, medium probability. Performance degradation under load: Medium impact, medium probability. Mitigation Strategies\nUse AWS multi-region and monitor with CloudWatch/X-Ray. Test and review source code before deployment via CodePipeline. Set up cost alerts via AWS Budgets. Perform automated security scanning using CodeBuild (replacing GitHub Actions). Contingency Plan\nMaintain a staging environment for quick recovery. Use CloudFormation and AWS Backup for configuration and data backup. 8. Expected Outcomes Technical Improvements\nComprehensive Automation: Complete transition to automated DevSecOps process. New system deployment time takes under 6 minutes. Performance Guarantee: Fast application operation (API response under 150ms) and stability (99.9% Uptime) thanks to Serverless architecture. Integrated Security: Automated scanning and remediation of high-level security vulnerabilities during the Code Build process. Scalability Ready: The platform can scale to serve many users and handle large traffic volumes without structural changes. Long-term Value\nTechnical Asset Creation: Creation of a complete AWS CDK/CloudFormation codebase. This is a cost-optimized Serverless architecture template that can be reused for other projects by the team. Robust Platform Foundation: Establishment of a work management and development environment following industrial standards (DevSecOps), ready for future feature expansion. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Workshop Reflection Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Purpose of the Event Provide a comprehensive overview of the AI/ML/GenAI ecosystem on AWS, focusing on core services. Equip participants with practical knowledge of the end-to-end AI/ML deployment process from data preparation to model operations using Amazon SageMaker. Offer hands-on guidance in applying Generative AI through Amazon Bedrock, including prompt engineering, RAG, and agent building. Create a networking and learning environment for students and developers interested in cloud-based AI/ML. Main Speakers / Instructors (Based on Program Content) Mr. Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam (Introduction \u0026amp; Orientation) Ms. Trần Thị Minh Anh – Sr. AI/ML Specialist Solutions Architect, AWS (Technical Presentation \u0026amp; Demo) Mr. Lê Quang Huy – Cloud Engineer \u0026amp; AI Enthusiast (Hands-on Support \u0026amp; Q\u0026amp;A) Highlights \u0026amp; Key Learnings 1. Overview of the AI/ML/GenAI Ecosystem on AWS Landscape in Vietnam: AI application is booming, from startups to large enterprises, with high demand for skilled professionals. AWS AI/ML Learning Path: From fundamentals (AWS AI/ML Fundamentals) to specialized certifications, emphasizing \u0026ldquo;learning by doing\u0026rdquo; through hands-on labs and projects. Three Main Pillars: AI Services (Ready-to-use), ML Services (Customizable), and Generative AI (Innovative). 2. Amazon SageMaker: The Comprehensive ML Platform End-to-end ML platform: A unified workspace for the entire ML lifecycle: from data preparation → model training → deployment → monitoring. SageMaker Studio: An integrated IDE experience with full tooling, significantly accelerating development speed compared to building custom workflows. Built-in MLOps: Automates training, tuning (AutoML), and model lifecycle management, ensuring models remain stable and effective in production. 3. Generative AI with Amazon Bedrock: From Theory to Practical Application Foundation Models (FMs): Comparing strengths/weaknesses of top models (Anthropic Claude, Meta Llama, Amazon Titan) and how to select the right model for specific use cases (cost, performance, task type). Advanced Prompt Engineering: Beyond \u0026ldquo;asking the right question,\u0026rdquo; it\u0026rsquo;s about structuring prompts effectively (Chain-of-Thought, Few-shot learning) for optimal results. Retrieval-Augmented Generation (RAG): Architecture connecting LLMs with private knowledge bases, solving the hallucination problem and updating models with new information. Bedrock Agents: Transforming LLMs into \u0026ldquo;intelligent assistants\u0026rdquo; capable of executing multi-step workflows, calling APIs, and querying databases autonomously. Guardrails for Bedrock: Mechanisms for filtering harmful content and ensuring safety and compliance with enterprise policies. Key Takeaways Technical Knowledge SageMaker is more than a training tool: It\u0026rsquo;s an ML lifecycle management platform that simplifies work for Data Scientists and ML Engineers. GenAI is not \u0026ldquo;magic\u0026rdquo;: Successful application requires understanding model fundamentals, prompt techniques, and supporting architectures like RAG. Data is paramount: Regardless of advanced technology, input data quality determines 80% of the AI model\u0026rsquo;s output quality. Mindset \u0026amp; Methodology Business Problem First: Always start with a specific business problem, then find suitable AI/ML technology to solve it. Iterative Development: Build quick prototypes with Bedrock, gather feedback, then invest in custom model training if needed. Cost Awareness: Calculate costs early (training, inference, storage) to ensure project feasibility. Practical Skills Prompt Engineering is a trainable skill: Requires understanding principles and practicing with various tasks. Knowing how to use AWS Console \u0026amp; SDK for SageMaker and Bedrock is a crucial first step for real projects. Personal Action Plan After the Workshop Practice immediately on AWS Free Tier: Create an AWS account (if not already) and start with SageMaker Studio Lab for free to familiarize with the interface. Build my first GenAI application: Use Amazon Bedrock (in preview, request access) to build a simple chatbot answering questions about a topic I know well. Deepen knowledge via AWS Skill Builder: Enroll in courses like \u0026ldquo;Generative AI with Large Language Models\u0026rdquo; and the \u0026ldquo;Machine Learning Learning Plan\u0026rdquo; to systematize knowledge. Join the community: Become more active in the AWS AI/ML Enthusiasts Vietnam group on Facebook/LinkedIn to learn from real-world case studies. Personal Experience at the Event The \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop truly opened a new horizon on approaching AI technology in a systematic and feasible way. The professional and modern setting at the AWS office, combined with the enthusiastic speakers, created a session that was both in-depth and friendly.\nThe live demo was an absolute highlight. Witnessing Ms. Minh Anh transform concepts like RAG and Bedrock Agents from theory into a working chatbot within minutes made many of us, including myself, have \u0026ldquo;ah-ha\u0026rdquo; moments of excitement. It proved that the gap from idea to prototype in GenAI is now very short, if we know how to use the right tools.\nI was also impressed by the practical perspective on cost and MLOps. The speakers didn\u0026rsquo;t just talk about the bright side of AI but also openly shared challenges in model management, monitoring, and operational costs—topics often less covered in academic settings.\nThe workshop ended with a feeling of \u0026ldquo;I can do this.\u0026rdquo; Instead of being overwhelmed by complex terminology, I felt I had a clear roadmap to start my exploration of AI/ML on AWS, beginning with the smallest practical steps.\nSome photos from the event "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about Amazon VPC and its core components such as Subnet, Internet Gateway, and NAT Gateway. Learn about key security concepts such as Security Group and NACL. Get familiar with launching EC2 instances and basic networking configurations. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Watch the theoretical videos of Module 2 to get an overall understanding of VPC - Define concepts such as Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, and NACL - Review the VPC architecture model 15/09/2025 15/09/2025 https://youtu.be/O9Ac_vGHquM?si=wDWqr_lJUjK2csDt https://youtu.be/BPuD1l2hEQ4?si=Qaig6saCCVKiqe0H https://youtu.be/CXU8D3kyxIc?si=TiZqHi0uB5mCif3L 3 - Hands-on: + Create VPC + Create Subnet + Configure Route Table for the public subnet + Attach Internet Gateway to the VPC + Verify the route table to confirm IGW functionality 16/09/2025 16/09/2025 https://000003.awsstudygroup.com 4 - Learn about Security Groups in VPC - Learn about Network ACLs - Review inbound/outbound rules of Security Groups - Review NACL rules in the subnet 17/09/2025 17/09/2025 https://000003.awsstudygroup.com 5 - Hands-on: + Launch EC2 instance in the public subnet + Create key pair to connect to EC2 + Review EC2 Security Group + Review EC2 Route Table and subnet mapping 18/09/2025 18/09/2025 https://000003.awsstudygroup.com 6 - Hands-on: + Test SSH connection to EC2 + Final verification of Security Group rules - Summarize all Module 2 theoretical content 19/09/2025 19/09/2025 https://000003.awsstudygroup.com Week 2 Achievements: Overview:\nThis week I became familiar with fundamental networking concepts in AWS and learned how to build a simple VPC. The main focus was understanding how VPC creates an isolated environment, how subnets are divided, and how IGW, routing, and security work together. Theory learned:\nOverview of VPC and how it operates within AWS Subnet (Public / Private) and subnet segmentation Internet Gateway \u0026amp; Route Table Security Group \u0026amp; Network ACL VPC Peering, VPN, Direct Connect Hands-on labs:\nPracticed creating a VPC Created subnets according to the lab design (public/private) Created and attached an Internet Gateway to the VPC Created Route Table and configured routes for the subnets Reviewed VPC structure after deployment "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.8-cognito/5.8.2-password-policies/","title":"Configure Password Policies","tags":[],"description":"","content":"Configure Password Policies In this step, you will configure password policies to ensure strong password requirements for your users.\nNavigate to your User Pool in the Cognito console Go to Authentication methods tab Click Edit in the Password policy section Password Policy Configuration Configure the following password requirements:\nPassword length:\nMinimum length: 8 characters Maximum length: 256 characters Password complexity:\n✅ Require numbers ✅ Require special characters ✅ Require uppercase letters ✅ Require lowercase letters Save Configuration Review your password policy settings Click Save changes Verify Password Policy The password policy is now active. Users will need to create passwords that meet these requirements:\nExample of valid passwords:\nMySecurePass123! StrongPassword2024# TaskManager@2025 Example of invalid passwords:\npassword (no uppercase, numbers, special chars) 12345678 (no letters, special chars) Pass1! (too short) "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3FullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:DeleteBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketCORS\u0026#34;, \u0026#34;s3:PutBucketCORS\u0026#34;, \u0026#34;s3:GetBucketWebsite\u0026#34;, \u0026#34;s3:PutBucketWebsite\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:PutBucketVersioning\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateDistribution\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:GetDistributionConfig\u0026#34;, \u0026#34;cloudfront:UpdateDistribution\u0026#34;, \u0026#34;cloudfront:DeleteDistribution\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;cloudfront:CreateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:GetOriginAccessControl\u0026#34;, \u0026#34;cloudfront:UpdateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:DeleteOriginAccessControl\u0026#34;, \u0026#34;cloudfront:ListOriginAccessControls\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFAndShieldAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:DeleteWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34;, \u0026#34;wafv2:AssociateWebACL\u0026#34;, \u0026#34;wafv2:DisassociateWebACL\u0026#34;, \u0026#34;wafv2:CreateIPSet\u0026#34;, \u0026#34;wafv2:GetIPSet\u0026#34;, \u0026#34;wafv2:UpdateIPSet\u0026#34;, \u0026#34;wafv2:DeleteIPSet\u0026#34;, \u0026#34;wafv2:ListIPSets\u0026#34;, \u0026#34;wafv2:CreateRuleGroup\u0026#34;, \u0026#34;wafv2:GetRuleGroup\u0026#34;, \u0026#34;wafv2:UpdateRuleGroup\u0026#34;, \u0026#34;wafv2:DeleteRuleGroup\u0026#34;, \u0026#34;wafv2:ListRuleGroups\u0026#34;, \u0026#34;shield:DescribeSubscription\u0026#34;, \u0026#34;shield:GetSubscriptionState\u0026#34;, \u0026#34;shield:DescribeProtection\u0026#34;, \u0026#34;shield:ListProtections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;cognito-idp:DescribeUserPool\u0026#34;, \u0026#34;cognito-idp:ListUserPools\u0026#34;, \u0026#34;cognito-idp:UpdateUserPool\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolClient\u0026#34;, \u0026#34;cognito-idp:UpdateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:ListUserPoolClients\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-identity:CreateIdentityPool\u0026#34;, \u0026#34;cognito-identity:DeleteIdentityPool\u0026#34;, \u0026#34;cognito-identity:DescribeIdentityPool\u0026#34;, \u0026#34;cognito-identity:UpdateIdentityPool\u0026#34;, \u0026#34;cognito-identity:ListIdentityPools\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;APIGatewayFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34;, \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:PUT\u0026#34;, \u0026#34;apigateway:PATCH\u0026#34;, \u0026#34;apigateway:DELETE\u0026#34;, \u0026#34;apigateway:UpdateRestApiPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:UpdateFunctionConfiguration\u0026#34;, \u0026#34;lambda:PublishVersion\u0026#34;, \u0026#34;lambda:CreateAlias\u0026#34;, \u0026#34;lambda:UpdateAlias\u0026#34;, \u0026#34;lambda:DeleteAlias\u0026#34;, \u0026#34;lambda:GetAlias\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:AddPermission\u0026#34;, \u0026#34;lambda:RemovePermission\u0026#34;, \u0026#34;lambda:GetPolicy\u0026#34;, \u0026#34;lambda:PutFunctionConcurrency\u0026#34;, \u0026#34;lambda:DeleteFunctionConcurrency\u0026#34;, \u0026#34;lambda:TagResource\u0026#34;, \u0026#34;lambda:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:CreateTable\u0026#34;, \u0026#34;dynamodb:DeleteTable\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34;, \u0026#34;dynamodb:DescribeTimeToLive\u0026#34;, \u0026#34;dynamodb:UpdateTimeToLive\u0026#34;, \u0026#34;dynamodb:DescribeContinuousBackups\u0026#34;, \u0026#34;dynamodb:UpdateContinuousBackups\u0026#34;, \u0026#34;dynamodb:TagResource\u0026#34;, \u0026#34;dynamodb:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;KMSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateKey\u0026#34;, \u0026#34;kms:CreateAlias\u0026#34;, \u0026#34;kms:DeleteAlias\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListKeys\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:PutKeyPolicy\u0026#34;, \u0026#34;kms:GetKeyPolicy\u0026#34;, \u0026#34;kms:EnableKey\u0026#34;, \u0026#34;kms:DisableKey\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsManagerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:PutSecretValue\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodePipelineAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codepipeline:CreatePipeline\u0026#34;, \u0026#34;codepipeline:DeletePipeline\u0026#34;, \u0026#34;codepipeline:GetPipeline\u0026#34;, \u0026#34;codepipeline:GetPipelineState\u0026#34;, \u0026#34;codepipeline:UpdatePipeline\u0026#34;, \u0026#34;codepipeline:ListPipelines\u0026#34;, \u0026#34;codepipeline:StartPipelineExecution\u0026#34;, \u0026#34;codepipeline:StopPipelineExecution\u0026#34;, \u0026#34;codepipeline:GetPipelineExecution\u0026#34;, \u0026#34;codepipeline:ListPipelineExecutions\u0026#34;, \u0026#34;codepipeline:TagResource\u0026#34;, \u0026#34;codepipeline:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeBuildAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateProject\u0026#34;, \u0026#34;codebuild:DeleteProject\u0026#34;, \u0026#34;codebuild:UpdateProject\u0026#34;, \u0026#34;codebuild:BatchGetProjects\u0026#34;, \u0026#34;codebuild:ListProjects\u0026#34;, \u0026#34;codebuild:StartBuild\u0026#34;, \u0026#34;codebuild:StopBuild\u0026#34;, \u0026#34;codebuild:BatchGetBuilds\u0026#34;, \u0026#34;codebuild:ListBuildsForProject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeGuruReviewerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codeguru-reviewer:AssociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeRepositoryAssociation\u0026#34;, \u0026#34;codeguru-reviewer:ListRepositoryAssociations\u0026#34;, \u0026#34;codeguru-reviewer:DisassociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeCodeReview\u0026#34;, \u0026#34;codeguru-reviewer:ListCodeReviews\u0026#34;, \u0026#34;codeguru-reviewer:ListRecommendations\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFormationAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:UpdateStack\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;cloudformation:GetTemplate\u0026#34;, \u0026#34;cloudformation:ValidateTemplate\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudformation:ListStackResources\u0026#34;, \u0026#34;cloudformation:CreateChangeSet\u0026#34;, \u0026#34;cloudformation:DeleteChangeSet\u0026#34;, \u0026#34;cloudformation:DescribeChangeSet\u0026#34;, \u0026#34;cloudformation:ExecuteChangeSet\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DeleteLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:GetLogEvents\u0026#34;, \u0026#34;logs:FilterLogEvents\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;logs:DeleteRetentionPolicy\u0026#34;, \u0026#34;logs:TagLogGroup\u0026#34;, \u0026#34;logs:UntagLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;XRayAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;xray:PutTraceSegments\u0026#34;, \u0026#34;xray:PutTelemetryRecords\u0026#34;, \u0026#34;xray:GetSamplingRules\u0026#34;, \u0026#34;xray:GetSamplingTargets\u0026#34;, \u0026#34;xray:GetServiceGraph\u0026#34;, \u0026#34;xray:GetTraceSummaries\u0026#34;, \u0026#34;xray:GetTraceGraph\u0026#34;, \u0026#34;xray:BatchGetTraces\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SNSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sns:CreateTopic\u0026#34;, \u0026#34;sns:DeleteTopic\u0026#34;, \u0026#34;sns:GetTopicAttributes\u0026#34;, \u0026#34;sns:SetTopicAttributes\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;sns:Subscribe\u0026#34;, \u0026#34;sns:Unsubscribe\u0026#34;, \u0026#34;sns:ListSubscriptions\u0026#34;, \u0026#34;sns:ListSubscriptionsByTopic\u0026#34;, \u0026#34;sns:Publish\u0026#34;, \u0026#34;sns:TagResource\u0026#34;, \u0026#34;sns:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPassRoleForServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PassedToService\u0026#34;: [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;apigateway.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;cloudformation.amazonaws.com\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMRoleManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:UpdateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:ListAttachedRolePolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Workshop Reflection Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Purpose of the Event Provide a comprehensive understanding of DevOps culture, principles, and practices on the AWS platform. Guide participants in building complete CI/CD pipelines from source control to automated deployment. Introduce key Infrastructure as Code (IaC) tools on AWS and their practical applications. Equip participants with knowledge about containerization, observability, and best practices for stable system operations. Main Speakers / Instructors (Based on Program) Mr. Đỗ Huy Thắng – DevOps Lead, VNG (Core session on DevOps culture \u0026amp; practices) Ms. Nguyễn Thị Thu Hà – Sr. DevOps Engineer, AWS (Technical sessions on CI/CD \u0026amp; IaC) Mr. Phạm Tuấn Anh – Solutions Architect, AWS (Container \u0026amp; Observability sessions) Highlights \u0026amp; Key Learnings 1. DevOps Culture \u0026amp; Principles DevOps Mindset: Not just tools, but a collaborative culture between Development and Operations to optimize the entire software delivery process. Performance Measurement: DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) are critical indicators for assessing DevOps effectiveness. Shift-Left Testing \u0026amp; Security: Integrate testing and security early in the pipeline to detect issues faster and reduce remediation costs. 2. CI/CD Pipeline on AWS: From Code to Production AWS Developer Tools Suite: Comprehensive toolset (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) enabling full automation of the release process. Deployment Strategies: Deep understanding of Blue/Green (zero-downtime), Canary (risk reduction), and Rolling Updates for appropriate selection. Practical Demo: Building a complete pipeline from code commit, automated testing, to deployment on EC2/ECS, providing clear visualization of the workflow. 3. Infrastructure as Code (IaC): Managing Infrastructure as Source Code AWS CloudFormation: Powerful \u0026ldquo;declarative\u0026rdquo; IaC tool using YAML/JSON templates to define and manage AWS resources. AWS CDK (Cloud Development Kit): \u0026ldquo;Imperative\u0026rdquo; approach using familiar programming languages (Python, TypeScript) to define infrastructure, enhancing reusability and maintainability. Drift Detection: Mechanism to identify differences between code-defined configuration and actual infrastructure, ensuring consistency and compliance. 4. Container Services \u0026amp; Microservices Architecture Containerization with Docker: Packaging applications into lightweight, independent, and portable containers, the foundation of microservices architecture. Amazon ECS vs Amazon EKS: Detailed comparison of AWS\u0026rsquo;s top container orchestration services to choose the right tool for specific needs (fully-managed vs self-managed Kubernetes). AWS App Runner: \u0026ldquo;Serverless\u0026rdquo; solution simplifying container deployment, minimizing infrastructure management overhead. 5. System Monitoring \u0026amp; Observability Amazon CloudWatch: Central hub for collecting metrics, logs, setting alarms, and dashboards to monitor system health in real-time. AWS X-Ray: Performance analysis and debugging tool for distributed applications by tracing requests across services. Best Practices: Setting meaningful alerts, SLO-focused dashboards, and incident response processes (on-call, postmortems). Key Takeaways Mindset \u0026amp; Culture DevOps is a journey, not a destination: Requires continuous improvement based on data and feedback. Automation is core: Automate everything possible to reduce manual errors and free up time for creative work. Failure is an option: Build resilient systems and learn from incidents through blameless postmortems. Technical \u0026amp; Tools CI/CD is more than \u0026ldquo;running a pipeline\u0026rdquo;: It\u0026rsquo;s about creating a reliable workflow to deliver value to users quickly and safely. IaC transforms infrastructure management: Brings consistency, version control, and environment reproducibility. Observability \u0026gt; Monitoring: Not just alerting when things break, but understanding root causes through logs, metrics, and traces. Career Skills Need broad knowledge: A DevOps Engineer requires understanding from networking, security, coding to system operations. Problem-solving skills: The ability to debug complex systems is invaluable. Personal Action Plan After the Workshop Set up a personal CI/CD pipeline: Use AWS CodeCommit \u0026amp; CodePipeline to automate deployment of a static web application or small API to AWS. Get familiar with AWS CDK: Choose a language (Python) and write CDK code to deploy basic AWS resources (VPC, EC2, S3). Practice with Docker \u0026amp; ECS: Containerize a simple application into a Docker image, push to Amazon ECR, and attempt deployment on ECS Fargate. Build basic CloudWatch Dashboard: For my demo application, set up simple metrics and alarms. Personal Experience at the Event A full day from 8:30 AM to 5:00 PM focused on DevOps delivered an extremely valuable and practical wealth of knowledge. Compared to the AI/ML session, this workshop delved deeper into operational processes and techniques that developers like myself typically have less direct exposure to.\nThe CI/CD pipeline and IaC demos were particularly impressive. Witnessing how a single code commit triggered an entire automated sequence: build, test, security scan, then deployment with Blue/Green strategy, truly showcased the power of automation. It\u0026rsquo;s not just fast but significantly reduces risk.\nI also particularly enjoyed the sessions on DevOps culture and case studies. Hearing Mr. Thắng from VNG share how they applied DevOps to reduce MTTR from hours to minutes, or stories about startups failing due to neglecting monitoring, provided \u0026ldquo;priceless\u0026rdquo; lessons beyond textbooks.\nThe workshop concluded with the feeling of being equipped with a more complete \u0026ldquo;mental toolkit.\u0026rdquo; I realized that to become a proficient software engineer, beyond writing good code, one must understand the entire software lifecycle—from idea and code to operations and learning from incidents.\nSome photos from the event "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about EC2 within a VPC. Understand NAT Gateway \u0026amp; network security. Learn how to set up DNS and Route 53. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Review theory about EC2 in a VPC and NAT Gateway - Summarize the flow Public Subnet -\u0026gt; Private Subnet -\u0026gt; NAT -\u0026gt; IGW - Consolidate understanding of how NAT works 22/09/2025 22/09/2025 https://youtu.be/N58agSU4O8o?si=QVRiT5MDt9VN5el4 https://youtu.be/B1qxOQLmavQ?si=3ALVG-HGI0sbL8bM https://youtu.be/GVDsDu9dOFY?si=ki8qJYmPxwxLgYxd 3 - Hands-on: + Launch EC2 instance in the public subnet + Verify Security Group settings + Review subnet mapping and Route Table + Test EC2 Instance Connect 23/09/2025 23/09/2025 https://000003.awsstudygroup.com 4 - Hands-on: + Create NAT Gateway + Check private subnet routes + Test outbound traffic from private subnet to confirm NAT functionality 24/09/2025 24/09/2025 https://000003.awsstudygroup.com 5 - Learn about Hybrid DNS with Route 53 - Hands-on: + Create key pair + Create CloudFormation template 25/09/2025 25/09/2025 https://000010.awsstudygroup.com 6 - Hands-on: + Configure Security Groups + Connect to RDGW + Create Outbound and Inbound Endpoints + Create Resolver Rules and test DNS - Clean up all lab resources - Summarize EC2 – NAT – DNS concepts 26/09/2025 26/09/2025 https://000010.awsstudygroup.com Week 3 Achievements: Overview:\nThis week I learned how EC2 operates inside a VPC, understood the architecture of public/private subnets, NAT Gateway, and the security model. I also practiced extensively with EC2 and networking configurations. Theory learned:\nHow EC2 connects with subnets, route tables, and security groups How NAT Gateway enables Internet connectivity for private subnets How Security Groups control inbound/outbound traffic How NACL filters traffic at the subnet level DNS Resolver and how Route 53 handles hybrid DNS Hands-on labs:\nLaunched EC2 instances in different types of subnets Tested connectivity using EC2 Instance Connect Created NAT Gateway and verified outbound connections Continued practicing through DNS lab exercises to reinforce concepts "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.3-lambda/","title":"Lambda","tags":[],"description":"","content":"Initializing Lambda Create Lambda Function Select Create function → Author from scratch.\nIn the Basic information section, configure:\nFunction name: taskhub-backend-1 Runtime: .NET 8 (C#/F#/Powershell) Architecture: arm64 (default) In Permissions → Change default execution role:\nSelect Create a new role with basic Lambda permissions\n(AWS will automatically create a role with CloudWatch Logs write access) Keep the remaining settings as default and click Create function to finish.\nBuild Backend \u0026amp; Package into ZIP Because Lambda does not allow direct code editing for the .NET runtime, you must upload a ZIP file.\nOn your local machine, build the backend using: dotnet publish -c Release -o publish Open the publish folder → Select all files inside it, do not select the folder itself.\nCompress them into backend.zip Upload Source Code to Lambda In the Code tab → select Upload from → .zip file Select the file backend.zip\nClick Upload and wait for Lambda to deploy\nAfter the upload completes, the Code properties section will display the package size, SHA256 hash, and the last updated timestamp.\nVerify Runtime Settings In the Runtime settings section:\nRuntime: .NET 8 Handler: YourProject::YourNamespace.YourHandler::FunctionHandler\n(depends on your project structure) Architecture: arm64 Ensure that Lambda is running the correct entry point.\nConfigure Environment Variables Open the Configuration tab → Environment variables Click Edit → Add environment variable Add example variables:\nASPNETCORE_ENVIRONMENT = Production DynamoDB_TableName = Your_dynamoDB Jwt_Secret = Your_JWT_Secret Click Save.\nConfigure Timeout \u0026amp; Memory Open Configuration → General configuration\nClick Edit\nSet:\nMemory: 512MB – 1024MB Timeout: 30s (suitable for APIs) or higher if needed Click Save.\nAdd Trigger (API Gateway) To expose Lambda as an API with a public URL:\nGo to Configuration → Triggers Click Add trigger Choose:\nAPI Gateway Create an API REST API Security: Open (or IAM/Authorizer depending on your system)\nClick Add\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.8-cognito/5.8.3-email-verification/","title":"Setup Email Verification","tags":[],"description":"","content":"Setup Email Verification In this step, you will configure email verification to ensure users verify their email addresses during registration.\nNavigate to your User Pool in the Cognito console Go to Sign-up experience tab Click Edit in the Attribute verification and user account confirmation section Configure Email Verification Attribute verification and user account confirmation:\n✅ Send email message, verify email address Verification message: Code Email verification subject: Verify your email for Task Management System Email verification message: Your verification code for Task Management System is {####}. Please enter this code to complete your registration. Configure Email Delivery Email delivery method:\nSend email with Cognito (for development) FROM email address: no-reply@verificationemail.com Note: For production applications, consider using Amazon SES for better email deliverability and custom domains.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the blogs that have been translated. For example:\nBlog 1 - How Uniphore Achieved 30% Cost Savings by Modernizing Windows Servers on AWS This blog introduces Uniphore\u0026rsquo;s infrastructure modernization journey - from legacy Windows Server on-premises systems to cloud-native architecture on AWS. You\u0026rsquo;ll learn how Uniphore overcame challenges related to scalability, high maintenance costs, and security risks from end-of-support operating systems. The article details the solution using Amazon EKS for containerization, AWS DataSync for efficient data migration, and other AWS services like EC2, EFS, S3 to achieve 30% cost savings while improving performance, security, and operational capabilities.\nBlog 2 - Updates to the AWS MSSP Competency: Delivering Turnkey Security Solutions for Customers This blog introduces important updates to the AWS Managed Security Service Provider (MSSP) Competency, designed to help customers easily identify security partners that match their specific needs. You\u0026rsquo;ll learn about the 7 new competency categories including Infrastructure Security, Workload Security, Application Security, Data Protection, Identity \u0026amp; Access Management, Incident Response, and Cyber Recovery. The article explains how MSSP partners provide comprehensive end-to-end security solutions, combining native AWS services with third-party tools, helping businesses focus on core operations while maintaining strong security posture.\nBlog 3 - Unlocking Retail Intelligence by Transforming Data into Actionable Insights Using Generative AI with Amazon Q Business This blog introduces the Amazon Q Business for Retail Intelligence solution - a generative AI-powered assistant specifically designed for the retail industry. You\u0026rsquo;ll learn how this solution combines the capabilities of Amazon Q Business with Amazon QuickSight to transform complex retail data into actionable insights through natural language interface. The article demonstrates how different roles in retail (executives, merchandise managers, store managers, marketing analysts) can use the solution to optimize operations, improve customer service, and enhance data-driven decision making processes.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Workshop Reflection Report: \u0026ldquo;AWS Cloud Mastery Series #3: Following the AWS Well-Architected Security Pillar\u0026rdquo; Purpose of the Event Introduce the Well-Architected Framework, focusing on the Security Pillar as the foundation for all systems on AWS. Provide practical knowledge across the five critical areas of cloud security: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. Equip participants with a \u0026ldquo;Security by Design\u0026rdquo; mindset and core security models (Least Privilege, Zero Trust, Defense in Depth). Share common practices and lessons learned from real-world implementations at Vietnamese enterprises. Main Speakers / Instructors (Based on Program) Mr. Nguyễn Văn Tú – Sr. Security Specialist Solutions Architect, AWS Ms. Lê Thị Thanh Mai – Cloud Security Engineer, AWS Partner Mr. Đặng Quốc Bảo – Security Lead, Fintech Company (Guest speaker for case studies) Highlights \u0026amp; Key Learnings 1. Foundational Security Mindset in the Cloud Security is the \u0026ldquo;First Pillar\u0026rdquo;: Every architecture on AWS must start with security, not as an add-on feature. Core Principles: Least Privilege: Grant only the minimum permissions necessary to perform a task. Zero Trust: \u0026ldquo;Never trust, always verify\u0026rdquo; every access request, regardless of origin (inside or outside the network). Defense in Depth: Build multiple, overlapping layers of defense so if one fails, others still provide protection. Shared Responsibility Model: AWS is responsible for security of the cloud, while the customer is responsible for security in the cloud. Understanding this boundary is critical. 2. Pillar 1 – Identity \u0026amp; Access Management (IAM) Avoid long-term credentials: Prioritize using IAM Roles for EC2, Lambda, ECS instead of storing fixed keys. IAM Identity Center \u0026amp; SSO: Centralized access management for multiple AWS accounts and SaaS applications, attaching Permission Sets to groups. Organization-wide control: Use Service Control Policies (SCPs) and Permission Boundaries to establish \u0026ldquo;guardrails\u0026rdquo; and set maximum permission limits across the entire organization. Supporting tools: IAM Access Analyzer to detect unintentionally public resources. 3. Pillar 2 – Detection \u0026amp; Continuous Monitoring Comprehensive logging: Enable CloudTrail at the organization level (Organization Trail), combined with VPC Flow Logs, Load Balancer logs. Threat detection services: GuardDuty uses AI/ML to detect anomalous behavior; Security Hub aggregates findings from multiple sources. Detection-as-Code: Define detection rules using code (e.g., AWS Config Rules) to ensure consistency and reproducibility. Alerting automation: Use Amazon EventBridge to route security events and trigger automated responses. 4. Pillar 3 – Infrastructure Protection Secure network design: Apply VPC segmentation (Public, Private, Data subnets), place sensitive resources in private subnets. Security Groups (Stateful) vs NACLs (Stateless): Security Groups control traffic at the instance level, NACLs at the subnet level with simpler rules. Application/web layer protection: Use WAF, Shield to defend against DDoS attacks and filter malicious HTTP/HTTPS traffic. Basic workload security: Apply OS hardening, use IMDSv2 for EC2, and leverage built-in security features in ECS/EKS. 5. Pillar 4 – Data Protection Encryption by default: Enable encryption-at-rest (using AWS KMS) for S3, EBS, RDS, and encryption-in-transit (TLS) everywhere. Key and secret management: Understand KMS Key Policies and use Secrets Manager for automatic secret rotation (e.g., database passwords). Data classification: Apply tagging and guardrails to control access based on data sensitivity levels. 6. Pillar 5 – Incident Response (IR) Prepared playbooks: Have response scripts ready for common scenarios like: leaked keys, unintended S3 public exposure, EC2 malware infection. Standard IR process: Contain → Eradicate → Recover, along with evidence collection (snapshots, logs) for post-analysis. Automated response: Use AWS Lambda and Step Functions to automatically isolate instances, revoke IAM keys, or send alerts. Key Takeaways Mindset \u0026amp; Strategy \u0026ldquo;Security is job zero\u0026rdquo;: Security cannot be patched on after a system is running. It must be designed from the start. Risk-based prioritization: Focus security resources on the most critical assets and the highest likelihood threats. Measure and improve: Use Security Hub scores and other metrics to assess and continuously enhance your security posture. Technical Insights Reduce attack surface: The fewer public resources, the fewer points for attackers to exploit. Encrypt everything: Treat encryption as a mandatory standard, not an option. Automate security: The more security tasks are automated (configuration scanning, secret rotation, incident response), the better. Personal Action Plan After the Workshop Apply Least Privilege immediately: Review IAM policies in my personal/demo AWS account and remove unnecessary permissions. Enable detection services: Activate AWS GuardDuty and Security Hub in my free tier account to familiarize myself with findings. Practice encryption: Create a KMS key and try encrypting a new S3 bucket or EBS volume. Study the Shared Responsibility Model: Deepen my understanding of my responsibilities for each AWS service I use. Personal Experience at the Event The workshop on the Security Pillar completely changed my perspective on building systems in the cloud. Previously, I saw security as something complex, distant, and reserved for specialists. This session demystified it, transforming security into a logical, systematic framework with five clear pillars.\nThe demos on the IAM Policy Simulator and practical Incident Response scenarios were incredibly valuable. Seeing how a policy is validated before assignment, or the automated steps to isolate a compromised EC2 instance, made me realize modern security is a combination of strict policies and intelligent automation.\nI was also impressed by the speakers\u0026rsquo; constant emphasis that \u0026ldquo;Security is everyone\u0026rsquo;s job.\u0026rdquo; A developer writing insecure code or a SysOps misconfiguring a Security Group can both create vulnerabilities. The biggest lesson was that a security mindset must be ingrained throughout the entire software development lifecycle, from the first line of code to operations.\nThe session ended with a feeling of greater responsibility. I realized I need to proactively learn and apply security principles from my smallest projects, as a single misconfiguration in the cloud can lead to significant consequences within minutes.\nSome photos from the event "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in many events, each of which was a memorable experience filled with new, interesting, and useful knowledge, along with wonderful gifts and moments.\nEvent 1 Event Name: Kick-off AWS First Cloud Journey Workforce - FPTU OJT FALL 2025\nTime: 08:30 AM, September 06, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in the event: OJT Program Student Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nTime: 08:30 AM - 12:00 PM, November 15, 2025\nLocation: AWS Vietnam Office, 26th Floor, Bitexco Financial Tower, District 1, Ho Chi Minh City\nRole in the event: Workshop Participant\nEvent 3 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nTime: 08:30 AM - 05:00 PM, November 17, 2025\nLocation: AWS Vietnam, 26th Floor, Bitexco Financial Tower, District 1, Ho Chi Minh City\nRole in the event: Workshop Participant\nEvent 4 Event Name: AWS Cloud Mastery Series #3: Following the AWS Well-Architected Security Pillar\nTime: 08:30 AM - 12:00 PM, November 29, 2025\nLocation: AWS Vietnam Office, 26th Floor, Bitexco Financial Tower, District 1, Ho Chi Minh City\nRole in the event: Workshop Participant\nEvent 5 Event Name: [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering\nTime: 02:00 PM, October 03, 2025\nLocation: AWS Event Hall, L26 Bitexco Financial Tower, District 1, Ho Chi Minh City\nRole in the event: Club Member Attendee\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn how to deploy VPC Peering between multiple VPCs. Learn how to configure Transit Gateway for large-scale inter-VPC connectivity. Practice advanced labs related to routing and DNS across VPCs. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the fundamentals of VPC Peering: how it works, limitations, routing behavior - Learn about the concept of peering inside a VPC 29/09/2025 29/09/2025 https://000019.awsstudygroup.com 3 - Hands-on: + Create EC2 instances in multiple VPCs and configure routing + Verify NACLs and Security Groups when using peering + Finalize the peering setup 30/09/2025 30/09/2025 https://000019.awsstudygroup.com 4 - Learn about Transit Gateway - Hands-on: + Deploy a TGW + Set up routing between VPCs and the TGW 01/10/2025 01/10/2025 https://000020.awsstudygroup.com 5 - Hands-on: + Create a Transit Gateway connection + Create a TGW Route Table + Map VPC routes to the TGW + Final connectivity check between multiple VPCs via TGW 02/10/2025 02/10/2025 https://000020.awsstudygroup.com 6 - Clean up all lab resources - Consolidate knowledge of Peering and Transit Gateway 03/10/2025 03/10/2025 https://000020.awsstudygroup.com Week 4 Achievements: Summary:\nThis week I explored multi-VPC connectivity models such as VPC Peering and Transit Gateway. I learned how inter-VPC networking works, how routing is handled via TGW, and how each component operates in a multi-VPC architecture. Theory learned:\nHow VPC Peering works and the routing mechanics between two VPCs Transit Gateway and its hub-and-spoke connectivity model How to configure VPC route tables when connecting through Peering or TGW Hands-on labs:\nCreate VPC Peering connections and update route tables Configure Transit Gateway, create attachments (TGW connections), and set up routing Verify connectivity between subnets/VPCs after configuration "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.4-apigateway/","title":"API Gateway","tags":[],"description":"","content":"Initializing API Gateway Create an API Gateway Function Open API Gateway\nSelect Create API Choose API type: REST API\nThen select: Build\nConfigure API Information Enter API name, for example: taskhub-backend-api\nEndpoint type: Regional\nSecurity policy: SecurityPolicy_TLS13_1_3_2025_09\nClick Create API\nCreate Resource for API In the API Gateway menu, select Resources\nClick Create resource\nEnter:\nResource name: auth, task, projects … depending on the project Resource path: /auth, /task, … Click Create resource\nCreate Method and Connect to Lambda Select a Resource → click Create method\nChoose ANY (or POST, GET depending on your API) Integration type: Lambda Function Tick Use Lambda Proxy integration Select Region Enter the name of the Lambda function, e.g., taskhub-backend_1 Click Save\nRepeat these steps to create additional API routes.\nGrant API Gateway Permission to Call Lambda Choose Deploy API\nCreate a new stage (if not available): prod1 Click Deploy Result: You will receive an Invoke URL like:\nhttps://ne6pw5hqej.execute-api.ap-southeast-1.amazonaws.com/prod1\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.8-cognito/5.8.4-app-client/","title":"Configure App Client","tags":[],"description":"","content":"Configure App Client When creating the User Pool, AWS Cognito automatically created a default App Client. In this step, we will configure this App Client to suit our application needs.\nNavigate to your User Pool in the Cognito console Go to App integration tab You will see the App Client has already been created Edit App Client Click on the App Client name to edit it Or click Edit if available Update app client information:\nApp client name: TaskManagementWebApp (if you want to change it) App client type: Public client Authentication flows: ✅ ALLOW_USER_PASSWORD_AUTH ✅ ALLOW_REFRESH_TOKEN_AUTH ✅ ALLOW_USER_SRP_AUTH Configure Hosted UI Hosted UI settings:\nUse the Cognito Hosted UI: Enabled Domain type: Use a Cognito domain Cognito domain: taskmanagement-auth-[your-unique-id] Initial app client settings:\nAllowed callback URLs: http://localhost:3000/callback https://your-app-domain.com/callback Allowed sign-out URLs: http://localhost:3000/ https://your-app-domain.com/ OAuth 2.0 settings:\nAllowed OAuth flows: ✅ Authorization code grant ✅ Implicit grant Allowed OAuth scopes: ✅ email ✅ openid ✅ profile Save Configuration Review all configurations Click Save changes Verify App Client Configuration After updating, note down the important information:\nApp Client Details:\nClient ID: [your-client-id] Hosted UI URL: https://taskmanagement-auth-[your-id].auth.us-east-1.amazoncognito.com The App Client is now configured and ready to handle authentication requests from your application. You can use the Client ID and Hosted UI URL to integrate with your web or mobile application.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Task Management Platform with DevOps on AWS Serverless Overview AWS Serverless enables you to build and deploy applications without managing servers, automatically scales based on demand, and you only pay for what you use.\nIn this workshop, we will learn how to design, build, and deploy a complete task management platform TaskHub using serverless architecture and automated DevSecOps practices.\nWe will create a system that includes frontend, backend API, database, and a complete CI/CD pipeline. The workshop focuses on three main components to build a production-ready application on AWS:\nServerless Backend - Use AWS Lambda for business logic processing, API Gateway as the communication layer, DynamoDB for data storage, and Cognito for user authentication management with optimized costs.\nContent Delivery - Deploy Next.js application on S3, distribute globally via CloudFront with low latency, and protect with AWS WAF against common web attacks.\nDevOps Pipeline - Automate the build, test, and deploy process using CodePipeline and CodeBuild, integrate security scanning with CodeGuru, and manage infrastructure as code with CloudFormation.\nContent Workshop overview Prerequiste Deploying Serverless Functions with AWS Lambda Building an API Gateway with Amazon API Gateway Simple and Secure Object Storage with Amazon S3 Accelerating Content Delivery with Amazon CloudFront (CDN) Managing User Identity and Access with Amazon Cognito Managing Encryption Keys with AWS Key Management Service (KMS) SecretManager WAF "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn the core components of EC2 and how compute operates in AWS. Understand Auto Scaling, EBS, Instance Store, User Data, Metadata. Practice backup, Storage Gateway, and deploying EC2 for storage-related use cases. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn about EC2, instance types, AMI, key pair - Understand EBS, Instance Store, User Data, Metadata 06/10/2025 06/10/2025 https://youtu.be/-t5h4N6vfBs?si=GeVdhO9IEDjzzS_D https://youtu.be/e7XeKdOVq40?si=T3I4pgPoEfVytcU3 https://youtu.be/yAR6QRT3N1k?si=GQghyBwLCpijrDON https://youtu.be/hKr_TfGP7NY?si=gR2MqaLAFrqL-KBo https://youtu.be/6IHNDJ85aoQ?si=M0puk6DJpliO7ahf https://youtu.be/_v_43Wi7zjo?si=qNDVWzKcQFNO2mGh https://youtu.be/Ew3QRaKJQSA?si=xNvXvD8yFhnSMJby 3 - Understand EC2 Auto Scaling and how VM scaling works - Learn about storage and compute services (EFS/FSx, Lightsail, MGN overview) 07/10/2025 07/10/2025 https://youtu.be/bbLcPitXJSY?si=eyVnxvL9ho0LpUYy https://youtu.be/hFVYG8WqfU0?si=9Px4wmR4IRZxk15n 4 - Hands-on: + Deploy AWS Backup + Create backup plan + Test restore \u0026amp; cleanup + Clean up backup 08/10/2025 08/10/2025 https://000013.awsstudygroup.com 5 - Hands-on: + Create an S3 bucket for Storage Gateway + Create EC2 for Storage Gateway + Create Storage Gateway + File Share + Clean up Storage Gateway 09/10/2025 09/10/2025 https://000024.awsstudygroup.com 6 - Hands-on: + Create bucket, upload data + Enable static website hosting + Configure public access block + Configure CloudFront and test website + Clean up website + CloudFront + bucket 10/10/2025 10/10/2025 https://000057.awsstudygroup.com Week 5 Achievements: Summary:\nThis week I learned how EC2 operates, different instance storage types, Auto Scaling, and backup mechanisms. I also practiced Storage Gateway and deployed an S3 static website. Theory learned:\nEC2 architecture, AMI, key pair EBS vs Instance Store User Data / Metadata EC2 Auto Scaling Storage Gateway and fundamentals of AWS Backup Hands-on labs:\nCreate backup plan + test restore Create Storage Gateway + file share Deploy static website using S3 + CloudFront "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Club Session Reflection Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Purpose of the Event Explore the transformation in software development driven by the rise of Generative AI. Introduce methods for integrating AI into the entire Software Development Life Cycle (SDLC): from architecture and development to testing, deployment, and maintenance. Experience and witness the power of AI-powered programming tools like Amazon Q Developer and Kiro in automating tasks, boosting productivity, and fostering creativity. Organizers \u0026amp; Speakers Main Instructors: Mr. Toan Huynh – Presentation on AI-Driven Development Life Cycle and Amazon Q Developer demo. Ms. My Nguyen – Kiro tool demonstration. Coordination Team: Ms. Diem My, Mr. Dai Truong, Mr. Dinh Nguyen (AWS GenAI Builder Club Vietnam). Highlights \u0026amp; Key Learnings 1. Overview of the AI-Driven Development Life Cycle Generative AI is a paradigm shift: Not merely an assistive tool, but actively reshaping how developers and organizations learn, plan, build, and operate software. Automating \u0026ldquo;undifferentiated heavy lifting\u0026rdquo;: AI can take over repetitive, time-consuming yet low-creativity tasks (e.g., writing boilerplate code, generating test cases, debugging common errors). Shifting developer roles: From writing every line of code to becoming architects, supervisors, and collaborators with AI, focusing on solving more complex and creative problems. 2. Amazon Q Developer: The Comprehensive AI Assistant for Developers Support across the entire SDLC: Amazon Q is more than a code-writing chatbot; it assists from: Planning \u0026amp; Design: Analyzing requirements, proposing architectures. Development: Generating code, explaining code, converting languages (Java upgrades, .NET modernization). Testing \u0026amp; Debugging: Creating test cases, finding root causes of errors. Deployment \u0026amp; Maintenance: Optimizing configurations, troubleshooting issues. Deep AWS integration: Understands AWS service contexts, optimizing development on the cloud platform. Security \u0026amp; Compliance: Built with privacy principles, does not use enterprise data to train public models. 3. Kiro: A Practical Tool for AI Pair-Programming Intuitive coding experience: The demo showed Kiro\u0026rsquo;s ability to understand complex code contexts, suggest intelligent edits, and even implement new features based on natural language descriptions. Accelerated development speed: Rapid and accurate code generation allows developers to focus on business logic rather than syntax. Continuous learning: Tools like Kiro can become \u0026ldquo;partners\u0026rdquo; helping developers learn new languages and frameworks more quickly. Key Takeaways Trends \u0026amp; Future Outlook AI doesn\u0026rsquo;t replace developers; it changes their jobs: Roles will shift more towards problem-solving, system design, and quality assurance than just writing code. Speed is the new competitive advantage: Organizations that effectively integrate AI into their SDLC will release products faster, with higher quality and lower cost. Skills to Develop \u0026ldquo;AI Communication\u0026rdquo; Skills (Prompt Engineering): The ability to clearly describe requirements, ask precise questions, and interact with AI will become a core competency. Architectural \u0026amp; Supervisory Thinking: The need to effectively evaluate, test, and integrate AI-generated code safely. Strong Foundational Knowledge: AI is just a tool. A skilled developer still needs to deeply understand programming principles, data structures, algorithms, and system architecture to give accurate commands and evaluate results. Personal Action Plan After the Event Sign up and experience Amazon Q Developer: Learn how to integrate it into your IDE (VSCode, JetBrains) and start using it for daily tasks like writing comments, generating simple functions, or debugging. Practice Prompt Engineering for Coding: Challenge yourself with exercises like: \u0026ldquo;Using Amazon Q, write a Python function to process data from an S3 bucket,\u0026rdquo; and refine prompts for optimal results. Engage with the AWS GenAI Builder Club community: Continue following and participating in future sessions to stay updated on new tools, case studies, and connect with like-minded individuals. Apply to a real project: Select a module in a personal or school group project to trial an \u0026ldquo;AI-Driven Development\u0026rdquo; process, documenting the effectiveness and lessons learned. Personal Experience at the Event This club session had a distinctly different atmosphere, focused on the very near future of programming. Witnessing Mr. Toan and Ms. My guiding AI tools to \u0026ldquo;converse\u0026rdquo; and build software felt like observing a leap similar to the shift from manual coding to using intelligent IDEs years ago.\nThe live demos were the absolute highlight. Seeing Amazon Q Developer not only answer questions but proactively suggest code improvements, analyze potential security vulnerabilities, or watching Kiro turn a plain English description into working code was genuinely impressive. It demonstrated the potential to significantly shorten the \u0026ldquo;time-to-code\u0026rdquo; for routine tasks.\nThe most resonant message was that \u0026ldquo;AI empowers, not replaces.\u0026rdquo; The speakers and coordinators emphasized that developers who leverage AI will have a tremendous advantage, and the biggest challenge isn\u0026rsquo;t the technology itself, but changing mindsets and workflows to welcome an AI teammate.\nThe session ended with much inspiration and some reflection. I realized I need to be more proactive in familiarizing myself and mastering these tools starting now, treating them as an indispensable part of the skillset for a future software engineer.\nSome photos from the event "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.6-s3/","title":"Set up S3","tags":[],"description":"","content":"WORKLOG: S3 ORIGIN CONFIGURATION (FULL) This Worklog outlines the steps for creating and configuring two (02) separate Amazon S3 Buckets, serving as the Origins for different system resources: Frontend Code and user-uploaded files.\n1. S3 Bucket 1 Configuration: taskhub-frontend-prod (Frontend Origin) This Bucket serves as the Origin for CloudFront, storing Frontend code (HTML, CSS, JS) and static assets.\n1.1. Access and Bucket Creation Log in to the AWS Console, find and select the Amazon S3 service. Click the \u0026ldquo;Create bucket\u0026rdquo; button. Fill in the general configuration information: Configuration Value Explanation Bucket name taskhub-frontend-prod The Bucket name must be globally unique. AWS Region Asia Pacific (Singapore) ap-southeast-1 Select the geographical region closest to the target users to optimize latency. 1.2. Object Ownership \u0026amp; Public Access Configuration Object Ownership: Select ACLs disabled (recommended). Purpose: Access permissions are managed centrally using a Bucket Policy, simplifying management. Block Public Access settings for this bucket: Action: Uncheck [ ] Block all public access (and all sub-options). Reason: This allows us to configure a Bucket Policy later to grant read-only access specifically to CloudFront OAC (Origin Access Control), ensuring S3 can function as a valid private Origin. 1.3. Versioning and Encryption Configuration Configuration Value Explanation Bucket Versioning Select: Disable Reduces Storage Costs as maintaining multiple versions of Frontend code is unnecessary. Default encryption Enable Ensures data is encrypted when stored (at rest). Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) The default, cost-effective encryption method. Bucket Key Select: Enable Minimizes Request Costs related to the encryption/decryption process. 2. S3 Bucket 2 Configuration: taskhub-files-prod (User Files Storage) This Bucket is used to store files uploaded by users (images, media\u0026hellip;). Maximum security is prioritized. The creation process is similar to the S3 Bucket above.\n2.1. Access and Bucket Creation Repeat the Bucket creation steps (Section 1.1). Bucket name: taskhub-files-prod AWS Region: Asia Pacific (Singapore) ap-southeast-1 2.2. Public Access Configuration (SECURITY DIFFERENCE) Block Public Access settings for this bucket: Keep [X] Block all public access (CHECK ALL). Reason: This is a Secure Bucket containing user data. Files MUST NOT be publicly accessible. Access will only be temporarily granted via Pre-signed URLs generated by the Backend API after authentication. 2.3. Versioning and Encryption Configuration (Similar) Configuration Value Explanation Bucket Versioning Select: Disable Prevents rapid storage cost increases when users update/delete files. Default encryption Enable Data encryption is mandatory for user data. Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) Standard S3 encryption. Bucket Key Select: Enable Reduces encryption request costs. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn the overview of AWS storage services (S3, Glacier, Backup, Storage Gateway, Snow Family). Understand how S3 works: access point, storage class, CORS, static website hosting. Practice the entire workflow with S3, Backup, Storage Gateway, and File Systems. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the overview of AWS storage services: S3, EBS, Backup, Storage Gateway, Snow Family - Study Access Point, Storage Class, and data access models - Understand S3 static website, CORS, Object key, Glacier 13/10/2025 13/10/2025 https://youtu.be/hsCfP0IxoaM?si=O3vMWs7Trr1fugJD https://youtu.be/_yunukwcAwc?si=ZhkTKr-_OkyUNImI https://youtu.be/mPBjB6Ltl_Q?si=qs6j0n7AeD2Mxwbz https://youtu.be/YXn8Q_Hpsu4?si=XojTnkR_LLC1KwEv 3 Hands-on: + Create S3 bucket + Deploy backup infrastructure + Create backup plan and set notification + Test restore \u0026amp; clean up backup resources 14/10/2025 14/10/2025 https://000013.awsstudygroup.com 4 - Learn VMware Workstation - Hands-on: + Export VM from on-prem + Upload VM to AWS + Import as EC2 + Export back as AMI + Clean up import/export environment 15/10/2025 15/10/2025 https://000014.awsstudygroup.com 5 - Hands-on: + Create Storage Gateway + Create advanced File Share + Connect File Share from on-prem machine + Clean up Storage Gateway + File Shares 16/10/2025 16/10/2025 https://000024.awsstudygroup.com 6 - Hands-on (lab25): + Create FSx file system (SSD/HDD, Multi-AZ) + Create \u0026amp; configure file shares + Test \u0026amp; monitor performance + Manage user sessions + quotas - Hands-on (lab57): + Create bucket, upload data, enable static website + Configure public access + object permissions + Create \u0026amp; configure CloudFront distribution + Enable versioning \u0026amp; object replication - Clean up environment (lab25), bucket, CloudFront, replication 17/10/2025 17/10/2025 https://000025.awsstudygroup.com https://000057.awsstudygroup.com Week 6 Achievements: Summary:\nThis week I learned the AWS storage ecosystem including S3, Glacier, Backup, Storage Gateway, and file systems. I focused heavily on hands-on labs to understand data management, backup–restore, and AWS storage mechanisms. Theory learned:\nS3 Storage Class, Access Point, CORS Glacier, lifecycle, backup concepts Storage Gateway and file system architecture VM import/export Hands-on labs:\nBackup \u0026amp; restore Import on-prem VM into AWS Create Multi-AZ file system Create static website, CloudFront, versioning, replication Practice Storage Gateway – create file share, connect, test data transfer between on-prem and AWS "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Internship Overview During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from September 08, 2025 to December 09, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired at university in a real-world working environment.\nI participated in learning and hands-on practice through intensive AWS cloud computing workshops, thereby improving my skills in working with cloud services, technology analysis, technical report writing, and professional communication.\nSelf-Evaluation Table No. Criteria Level Brief Comment 1 Professional Knowledge Fair Understand and can perform basic cloud and AI practical exercises. 2 Learning Ability Good Quickly absorb new knowledge from training sessions. 3 Proactiveness Fair Self-study beyond class hours, but could ask more questions proactively. 4 Responsibility Good Complete all assignments and reports on time. 5 Discipline Average Attend all sessions, sometimes need better personal time management. 6 Communication Fair Can exchange ideas within the group, need more confidence when speaking in front of a crowd. 7 Teamwork Good Collaborate well and support team members when needed. 8 Problem Solving Fair Can analyze problems, need more practice to find optimal solutions. 9 Attitude Good Serious, respectful towards mentors and peers. Strengths Quickly absorb new knowledge. Responsible with assigned tasks. Cooperative and supportive in teamwork. Areas for Improvement Communication Confidence: Need to be bolder when presenting opinions. Time Management: Better organize time for study and rest. In-depth Understanding: Should ask more questions to grasp topics thoroughly. Development Plan Actively participate in discussions and voice opinions. Create a detailed schedule for study and review. Proactively consult mentors when encountering unclear issues. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn the complete IAM system: user, group, role, policy, permission boundary. Understand AWS authentication \u0026amp; authorization mechanisms, how to write JSON policies, and how policy evaluation works. Get familiar with AWS Organizations, Organizational Units (OU), and Service Control Policies (SCP). Practice IAM + Organization labs to understand how large-scale account management works. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the IAM fundamentals: user, group, role, policy - Understand policy evaluation: explicit deny, implicit deny, allow 20/10/2025 20/10/2025 https://youtu.be/tsobAlSg19g?si=9f3mlIWPtrCcNuKg https://youtu.be/N_vlJGAqZxo?si=e8oiWCObco95CoKh 3 - Hands-on: + Create user/group/role + Attach inline \u0026amp; managed policies + Test S3/EC2 access under different policies - Learn permission boundaries and session policies 21/10/2025 21/10/2025 https://000028.awsstudygroup.com 4 - Study AWS Organizations: OU structure, multi-account setup - Understand SCP concepts, deny list vs allow list 22/10/2025 22/10/2025 https://youtu.be/5oQY8Rogz9Y?si=h8DlUb8ZLI4HbbvM https://youtu.be/NW1xrMkNMjU?si=dhT0T3y2JYVK8QwT 5 - Hands-on: + Create Organization + OU + Apply SCP deny EC2 / deny S3 + Verify SCP effectiveness combined with IAM policies + Rearrange OU, remove SCP 23/10/2025 23/10/2025 https://000030.awsstudygroup.com https://000044.awsstudygroup.com/ 6 - Team activity: + Discuss workshop ideas + Plan execution + Divide tasks for workshop 24/10/2025 24/10/2025 Week 7 Achievements: Summary:\nThis week I learned the foundation of AWS access management, including IAM and Organizations. I now understand how policy evaluation works, how to write JSON policies, and how SCPs apply across multi-account environments. Theory learned:\nConcepts of User – Group – Role – Policy and how evaluation works Inline policy, managed policy, permission boundary AWS Organizations structure, OU SCP concepts and differences compared to IAM policies Landing Zone \u0026amp; Control Tower overview Hands-on labs:\nCreate user/group/role and test different access levels Write JSON policies and test allow/deny behaviors Create Organization, OU, and apply SCP Validate SCP + IAM policy combinations in practice "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.7-cloudfront/","title":"Set up CloudFront","tags":[],"description":"","content":"COMPLETE CLOUDFRONT DISTRIBUTION CONFIGURATION This Worklog outlines the steps for configuring a CloudFront Distribution, focusing on setting up Origin Access Control (OAC) security and verifying the prerequisites on the S3 Bucket.\n1. S3 PREREQUISITES CHECK Before finalizing CloudFront setup, you must ensure the S3 Bucket taskhub-frontend-prod is protected and configured correctly.\n1.1. Static Website Hosting Status Check: The Properties tab of the S3 Bucket. Status: S3 static website hosting must be in the Disabled state. Explanation: Since CloudFront acts as the CDN, Static Website Hosting is not required on S3. S3 merely serves as the content repository (Origin). 1.2. Block Public Access \u0026amp; Bucket Policy Check: The Permissions tab of the S3 Bucket. Block public access (bucket settings): Must be in the On state (Block all public access). Bucket policy (Final Confirmation): Must contain the OAC Policy that is automatically updated by CloudFront . 2. CLOUDFRONT DISTRIBUTION CONFIGURATION 2.1. Step 1 \u0026amp; 2: Get started Configuration Value Explanation Action Click \u0026ldquo;Create distribution\u0026rdquo;. Start the CDN creation process. Plan Select Free Plan ($0/month). The free plan for the project. Distribution name taskhub-frontend-cdn A memorable name for the resource. Distribution type Single website or app. The appropriate type for a Frontend application. 2.2. Step 3: Specify origin (OAC Setup) 1. Specify Origin Origin type: Select Amazon S3. S3 origin: Select the S3 Bucket taskhub-frontend-prod. 2. Configure OAC (Automatic Security) 1. Tick \u0026ldquo;Allow private S3 bucket access to CloudFront - Recommended\u0026rdquo;. 2. Select Use recommended origin settings. * Explanation: Upon completing the Distribution creation, CloudFront will automatically update the S3 Bucket Policy to grant access via OAC.\n3. Cache Configuration Cache settings: Select Use recommended cache settings tailored to serving S3 content. Default Root Object: Enter index.html (Standard configuration for SPA). 2.3. Step 4 \u0026amp; 5: Security \u0026amp; TLS Configuration Applied Value Explanation WAF Uncheck paid features. Keeping defaults for the project. Viewer protocol policy Redirect HTTP to HTTPS Mandates the use of the secure protocol. Custom SSL certificate Default CloudFront Certificate Activates free HTTPS. 2.4. Step 6: Review and create 1. Review Origin (Source Confirmation) S3 origin: taskhub-frontend-prod. (Must ensure the correct Frontend Bucket is selected). Grant CloudFront access to origin: Must be Yes. (Confirms OAC functionality). 2. Final Configuration Price Class: Select Use all edge locations (best performance). Click the \u0026ldquo;Create distribution\u0026rdquo; button to deploy. 3. POST-DEPLOYMENT CHECK: OAC SECURITY CONFIRMATION After the Distribution status changes to Deploying, perform this crucial check to confirm that OAC is functional:\nAccess S3 Bucket taskhub-frontend-prod. Navigate to the \u0026ldquo;Permissions\u0026rdquo; tab. Find the \u0026ldquo;Bucket policy\u0026rdquo; section. Confirm: The Policy must be automatically updated and contain the JSON statement authorizing the CloudFront service. This confirms that OAC has blocked direct access and only allows CloudFront to read the files. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"General Assessment 1. Working Environment\nThe working environment in the FCJ program is very professional and supportive. The learning and working spaces are well-organized, helping me focus and absorb knowledge effectively. The workshops held at the AWS office gave me the feeling of working in a real-world environment.\n2. Support from Mentors / Team Admin\nMentors and team admin are always enthusiastic in their support. They answer all questions in detail and encourage independent thinking and self-learning. I particularly appreciate how mentors share practical experiences and provide career development guidance.\n3. Alignment between Work and Academic Major\nThe training content is highly relevant to my Information Technology major. Workshops on Cloud, AI/ML, and DevOps closely follow current technology trends, allowing me to apply what I\u0026rsquo;ve learned while acquiring new skills.\n4. Learning Opportunities \u0026amp; Skill Development\nI have learned many important skills: from technical ones (using AWS services) to soft skills (teamwork, presentation, report writing). The opportunity to attend sharing sessions from experts and alumni has been invaluable.\n5. Culture \u0026amp; Team Spirit\nThe culture at FCJ is very open and encourages learning. Everyone in the program is always ready to support each other. I truly feel the spirit of \u0026ldquo;sharing to grow together\u0026rdquo; among all participants.\n6. Policies / Benefits for Interns\nThe program provides excellent support for students, from offering AWS accounts for hands-on practice to organizing networking events with businesses. The small gifts during each workshop also provide great motivational boosts.\nAdditional Questions What are you most satisfied with during your internship?\nI\u0026rsquo;m most satisfied with the opportunity to access the latest technologies (AI/ML, GenAI) and learn from top experts. The chance to practice directly on AWS has significantly boosted my confidence.\nWhat do you think the program could improve for future interns?\nPerhaps adding a few more workshops (mini-projects) to apply all the knowledge learned comprehensively. This would help students gain deeper understanding of how services connect in real-world scenarios.\nWould you recommend this program to friends? Why?\nAbsolutely! This is a high-quality, free program that offers tremendous practical value for IT students. Not only do you learn about technology, but you also get to connect with the community and explore career opportunities.\nSuggestions \u0026amp; Expectations Would you like to continue with this program in the future?\nYes, very much! I hope there will be more advanced or specialized programs in specific fields to continue my development.\nOther comments (free to share):\nI would like to thank the organizers, mentors, and everyone involved for creating such a meaningful program. This has truly been an important stepping stone for my future career development journey.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn about database systems on AWS: RDS, Aurora, Redshift, ElastiCache. Practice building a database subnet group, test connectivity, backup \u0026amp; restore. Learn data analytics services such as Kinesis, Glue, Athena, QuickSight. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Learn about Databases: RDS, Aurora, Redshift, ElastiCache - Learn about Multi-AZ architecture, read replicas, backup/restore 27/10/2025 27/10/2025 https://youtu.be/OOD2RwWuLRw?si=9JsOs0PNfO1TdAUl https://youtu.be/qbrobQZrokY?si=ePJjzYXWg3qE_Ca6 https://youtu.be/UvdiRW34aNI?si=8g3FwgsJ3VLT-_nf 3 - Practice: + Create VPC + SG for EC2 + RDS + Create DB subnet group + Deploy EC2 + Create RDS instance + Backup \u0026amp; Restore 28/10/2025 28/10/2025 https://000005.awsstudygroup.com 4 - Practice: + Connect to MSSQL/Oracle + Schema Conversion + Create DMS Task + Inspect logs, troubleshoot 29/10/2025 29/10/2025 https://000043.awsstudygroup.com 5 - Learn about Data Analytics (Kinesis, Glue, Athena, QuickSight) - Practice: + Create DynamoDB table + Enable autoscaling + CRUD test + Create Global Table and clean up resources 30/10/2025 30/10/2025 https://000039.awsstudygroup.com 6 - Practice (lab35): + Create S3 bucket + Create Kinesis Firehose ingestion + Glue crawler + Query data with Athena + Create QuickSight dashboard - Practice (lab40): + Check cost allocation + Tagging resources + Additional queries \u0026amp; resource cleanup 31/10/2025 31/10/2025 https://000035.awsstudygroup.com https://000040.awsstudygroup.com Week 8 Achievements: Overview:\nThis week, I focused on AWS database and data analytics services, including RDS, Aurora, DynamoDB, DMS, Kinesis, Glue, Athena, and QuickSight. I gained a solid understanding of database architecture, connectivity, backup/restore, autoscaling, as well as the end-to-end data analytics pipeline from ingestion -\u0026gt; ETL -\u0026gt; query -\u0026gt; visualization. Theory Learned:\nConcepts of RDS, Aurora architecture, Multi-AZ, read replicas Backup, snapshot, parameter group, option group DynamoDB: partition key, sort key, throughput, autoscaling, DAX Overview of Data Analytics: Kinesis Firehose, Glue crawler, Athena, QuickSight Database Migration concepts: schema conversion, DMS task Lab Practice:\nCreated VPC + security groups for EC2/RDS Created DB subnet group, deployed EC2 and RDS MySQL Performed Backup \u0026amp; Restore Connected to MSSQL/Oracle, practiced Schema Conversion \u0026amp; created DMS task Created DynamoDB table, enabled autoscaling, CRUD test, created Global Table \u0026amp; cleanup Built analytics pipeline: Kinesis Firehose -\u0026gt; S3, Glue crawler, Athena queries, and QuickSight dashboard Performed additional tasks: tagging, cost allocation, and resource cleanup "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.8-cognito/","title":"Setup AWS Cognito Authentication","tags":[],"description":"","content":"AWS Cognito User Authentication Workshop Overview AWS Cognito provides user identity and access management for web and mobile applications. It enables you to add user sign-up, sign-in, and access control to your applications quickly and easily.\nIn this workshop, you will learn how to:\nCreate and configure a Cognito User Pool Setup password policies and email verification Configure App Client with Hosted UI Implement basic email/password authentication Content Create Cognito User Pool Configure Password Policies Setup Email Verification Configure App Client "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.9-keymanagementservice/","title":"Set up KMS","tags":[],"description":"","content":"Objective Create a Customer Managed Key (CMK) on AWS KMS to:\nEncrypt data in DynamoDB Encrypt Secrets Manager Ensure DevSecOps standard – data is encrypted using KMS Step 1 – Access AWS Key Management Service (KMS) Log in to AWS Console Search for service: KMS Select Key Management Service Go to menu Customer managed keys Click Create a key Step 2 – Configure Key At Key type, select Symmetric At Key usage, select Encrypt and decrypt Keep other options as default Click Next Step 3 – Add Labels (Alias \u0026amp; Description) Alias Enter:\ntaskhub_kms Click Next\nStep 4 – Define Key Administrative Permissions In the Key administrators list, select QuocBao Keep the option Allow key administrators to delete this key enabled Click Next The selected user at this step has full administrative permissions for the KMS Key:\nEdit key policy Enable / disable the key Delete the key Step 5 – Define Key Usage Permissions In the Key users list, select QuocBao No need to add Other AWS accounts Click Next Step 6 – Edit Key Policy At the Edit key policy step, click Edit Verify that the policy includes the following permission groups: Root account: kms:* User QuocBao is allowed to manage and use the key { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;key-consolepolicy-3\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow access for Key Administrators\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Create*\u0026#34;, \u0026#34;kms:Describe*\u0026#34;, \u0026#34;kms:Enable*\u0026#34;, \u0026#34;kms:List*\u0026#34;, \u0026#34;kms:Put*\u0026#34;, \u0026#34;kms:Update*\u0026#34;, \u0026#34;kms:Revoke*\u0026#34;, \u0026#34;kms:Disable*\u0026#34;, \u0026#34;kms:Get*\u0026#34;, \u0026#34;kms:Delete*\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:RotateKeyOnDemand\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow use of the key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow attachment of persistent resources\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:ListGrants\u0026#34;, \u0026#34;kms:RevokeGrant\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Bool\u0026#34;: { \u0026#34;kms:GrantIsForAWSResource\u0026#34;: \u0026#34;true\u0026#34; } } } ] } Modify the policy if necessary to match your actual account ARN Click Next AWS will automatically attach a valid policy to the key.\nStep 7 – Review \u0026amp; Finish Review all configurations: Item Value Key type Symmetric Key usage Encrypt and decrypt Alias taskhub_kms Key Admin QuocBao Key User QuocBao Click Finish AWS will start creating the KMS Key.\nStep 8 – Verify KMS Key Created Successfully After creation, go back to KMS → Customer managed keys Verify the following information: Alias: taskhub_kms Status: Enabled Key type: Symmetric Key spec: SYMMETRIC_DEFAULT Key usage: Encrypt and decrypt The KMS Key has now been successfully created.\n"},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn the overall Serverless architecture and its core components: API Gateway, Lambda, DynamoDB. Design the full application architecture: backend, frontend, authentication, security, and integrations. Explore the security and edge layer: CloudFront, Route 53, WAF. Learn the authentication system with Amazon Cognito and how tokens are issued for APIs. Analyze how CI/CD is implemented using CodePipeline, CodeBuild, GitLab, and IaC (CloudFormation). Learn AWS observability and monitoring systems: CloudWatch, X-Ray, SNS. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Learn Serverless architecture - API Gateway, Lambda, DynamoDB \u0026amp; integration mechanisms 03/11/2025 03/11/2025 3 - Design full architecture (BE + FE + Auth + Edge) - Draw architecture diagram \u0026amp; request flow 04/11/2025 04/11/2025 4 - Learn CloudFront, Route 53, WAF - Design security layer in front of API Gateway 05/11/2025 05/11/2025 5 - Learn Cognito (User Pool, Token) - Analyze how API Gateway validates JWT tokens 06/11/2025 06/11/2025 6 - Learn CI/CD: CloudFormation, CodePipeline, CodeBuild - Observability system: CloudWatch, X-Ray, SNS 07/11/2025 07/11/2025 Week 9 Achievements: Overview:\nThis week, I focused on understanding and designing the Serverless architecture for the application. I gained a solid understanding of how API Gateway – Lambda – DynamoDB work together, and explored the security layer using CloudFront/WAF as well as authentication via Cognito. I also learned the CI/CD workflow and logging/monitoring mechanisms to prepare for the coding phases in the upcoming weeks. Theory Learned:\nServerless architecture, pay-per-use model, and autoscaling principles. API Gateway REST API, Lambda integration, and DynamoDB table workflow. CloudFront + WAF + Route 53 for API protection. Cognito User Pool \u0026amp; Tokens (ID/Access), JWT flow through API Gateway authorizer. CI/CD with CodePipeline + CodeBuild + CloudFormation. CloudWatch logs/metrics, SNS alerts, tracing using X-Ray. Practice / Deliverables:\nFull system architecture diagram (BE – FE – Auth – Edge). Request flow from client -\u0026gt; CloudFront -\u0026gt; API Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB. Security diagram: CDN, DNS, WAF, throttling \u0026amp; protection at API Gateway. Preliminary CI/CD pipeline design using CloudFormation \u0026amp; CodePipeline. Authentication flow design: Cognito -\u0026gt; API Gateway JWT Authorizer. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Complete 100% of the Backend CRUD using .NET running on Aspire AppHost. Build and test the DynamoDB data model on the local environment (Docker). Integrate DynamoDB Local with NoSQL Workbench to validate the data model. Collaborate with the FE team to complete the basic UI and connect to local APIs. Prepare a solid foundation before moving into Week 11 (migrating code to Lambda + API Gateway). Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Verify \u0026amp; reinstall BE environment: .NET SDK, Docker Desktop, NoSQL Workbench - Understand system architecture diagram and backend processing flow 10/11/2025 10/11/2025 3 - Set up DynamoDB Local using Docker - Connect \u0026amp; visualize via NoSQL Workbench, manually test CRUD operations 11/11/2025 11/11/2025 4 - Implement DAL/Repository using .NET + AWSSDK.DynamoDBv2 - Integrate DI into Aspire AppHost - Implement business logic in BLL 12/11/2025 12/11/2025 5 - Build CRUD API Controllers - Perform unit tests \u0026amp; integration tests in the local environment 13/11/2025 13/11/2025 6 - Collaborate with the FE team - Build basic UI (List/Create) - FE connects to Local API and performs end-to-end testing 14/11/2025 14/11/2025 Week 10 Achievements: Overview:\nThis week, I completed the entire backend running locally using Aspire AppHost, built the DynamoDB data model, tested CRUD operations, and integrated with the frontend. By ensuring a complete and stable local architecture and codebase, I established a strong foundation for transitioning to Lambda/API Gateway in Week 11. Theoretical knowledge acquired:\nDynamoDB data modeling based on Single-Table Design, Access Patterns, PK/SK. Knowledge about DynamoDB Local, NoSQL Workbench, Docker setup. Using AWSSDK.DynamoDBv2 in .NET and integrating Dependency Injection. Backend structure in ASP.NET (Controller -\u0026gt; BLL -\u0026gt; Repository). How FE calls local APIs, handles responses, and renders UI. Practical work / Deliverables:\nSet up DynamoDB Local using Docker and managed schema via NoSQL Workbench. Built Repository + BLL + Controllers using .NET 8 / Aspire AppHost. Completed 5–6 CRUD APIs (POST/GET/PUT/DELETE/List). Wrote unit tests for BLL and integration tests for API. FE team built List + Create UI and successfully tested with local API. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete the remaining FE interface together with the Frontend team. Coordinate end-to-end testing between FE and BE. Learn the overall deployment workflow of the system (API, FE, database, infrastructure) to gain general understanding, even if not directly responsible. Prepare the necessary notes for Week 12 (documentation \u0026amp; final summary). Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Work with the FE team to complete the remaining UI components - Review API contract alignment between FE and BE 17/11/2025 17/11/2025 3 - FE integrates full CRUD API flows - Fix schema, payload, and status code mismatches during FE testing 18/11/2025 18/11/2025 4 - Perform full end-to-end testing: List -\u0026gt; Create -\u0026gt; Update -\u0026gt; Delete from FE to BE - Update response models/validations to match FE expectations 19/11/2025 19/11/2025 \u0026lt; \u0026gt; 5 - Learn the team’s deployment workflow (CI/CD, API Gateway, Lambda, S3 + CloudFront) - Take notes for Week 12 documentation 20/11/2025 20/11/2025 6 - Summarize FE and BE issues during the week - Review the entire BE codebase to prepare for deployment environment (even though not the one deploying) 21/11/2025 21/11/2025 Week 11 Achievements: Overview:\nThis week, I mainly collaborated with the FE team to finalize UI components and complete API integration. Since the backend was completed in Week 10, I helped fix issues, standardize API contracts, and conduct full end-to-end testing.\nAdditionally, I learned the team’s deployment workflow (Lambda, API Gateway, S3/CloudFront, CI/CD) to prepare documentation for Week 12. Theoretical knowledge acquired:\nHow FE calls CRUD APIs and debugs requests. API Contract: input/output schemas, error formats, status codes. Deployment workflow: .NET -\u0026gt; Lambda, API Gateway routing, FE build -\u0026gt; S3/CloudFront. Overview of CI/CD pipeline and how to prepare code for deployment (configs, logging). Practical work / Deliverables:\nCollaborated with FE to complete List/Create/Update/Delete screens. Adjusted backend according to FE requirements (schema, validation, status codes). Conducted full end-to-end testing between FE ↔ BE on local environment. Documented deployment workflow for use in Week 12. Compiled FE and BE issues and updated the team backlog. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and evaluate system performance after deployment on AWS. Optimize source code, Lambda functions, and DynamoDB database. Review basic authentication/authorization flows to avoid issues (no deep security work). Finalize the project summary report and prepare presentation materials. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Analyze CloudWatch logs - Use X-Ray to identify bottlenecks and runtime errors 24/11/2025 24/11/2025 3 - Check backend performance (API response time / Cold Start) - Perform light load testing to evaluate scalability 25/11/2025 25/11/2025 4 - Optimize .NET code: reduce package size, improve Lambda handlers - Review DynamoDB queries and optimize GSI if necessary 26/11/2025 26/11/2025 5 - Review basic authentication/authorization (no deep dive, only ensure stable functionality) - Finalize architecture diagram \u0026amp; data flow description 27/11/2025 27/11/2025 6 - Consolidate worklogs from Week 10–12 into the final report - Write evaluation, conclusion, strengths/weaknesses of the architecture - Prepare presentation slides 28/11/2025 28/11/2025 Week 12 Achievements: Overview:\nThis week, I focused on reviewing the entire system deployed on AWS, evaluating real performance, and optimizing key components. After that, I completed the documentation and final project report. Theoretical knowledge acquired:\nHow to query and analyze runtime logs in CloudWatch. Trace request flows with X-Ray to detect bottlenecks. Performance optimization for Lambda (cold start, initialization improvements). DynamoDB optimization using GSI/LSI and analyzing query access patterns. Summary of Serverless architecture and evaluation of pros/cons. Practical work / Deliverables:\nAnalyzed runtime errors and performance metrics via CloudWatch \u0026amp; X-Ray. Checked API response time and proposed cold-start improvements (if necessary). Reduced Lambda package size and improved execution performance. Optimized DynamoDB data model to reduce query latency. Finalized AWS architecture diagram. Completed the project summary report, including final evaluation and presentation slides. "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.17-secretmanager/","title":"Configure AWS Secrets Manager","tags":[],"description":"","content":"Objective Use AWS Secrets Manager to store configuration/secrets for the TaskHub system with the following requirements:\nSecrets are stored in JSON format (Key/value pairs – Plaintext) Data is encrypted using KMS CMK: taskhub_kms (Optional) Enable automatic secret rotation using AWS Lambda Step 1 – Access AWS Secrets Manager In the AWS Console, type Secrets Manager in the search box. Select Secrets Manager from the Services list. Step 2 – Create a New Secret (Key/value pairs – JSON) On the Secrets Manager main page, click Store a new secret. In Secret type, select: Other type of secret.\nIn the Key/value pairs section:\nSwitch from the Key/value tab to the Plaintext tab. Paste the following JSON content (demo for DynamoDB + KMS): { \u0026#34;Service\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;Table\u0026#34;: \u0026#34;TaskHub Tables\u0026#34;, \u0026#34;Encryption\u0026#34;: \u0026#34;SSE-KMS\u0026#34;, \u0026#34;KMSKeyAlias\u0026#34;: \u0026#34;taskhub_kms\u0026#34;, \u0026#34;Purpose\u0026#34;: \u0026#34;Store users, projects, tasks\u0026#34;, \u0026#34;DataProtection\u0026#34;: \u0026#34;Encrypted at rest\u0026#34; } In the Encryption key field, select the KMS key:\ntaskhub_kms Click Next.\nStep 3 – Configure Secret Name and Basic Information At the Configure secret step:\nSecret name:\nExample:\nprod/taskhub/secretmanager\nDescription (optional):\nMetadata for TaskHub DynamoDB encryption demo\nTags (optional): Skip for the workshop.\nResource permissions (optional): Keep default (IAM-based access control).\nReplicate secret (optional): Do not enable for this workshop.\nClick Next.\nStep 4 – Configure Automatic Rotation (Optional) In a production environment, secrets are usually rotated every 30 days.\nIn this workshop, a shorter interval is configured for demonstration purposes.\nAt Configure rotation – optional, enable: Automatic rotation In Rotation schedule: Select Schedule expression builder Time unit: Hours Hours: 23 (Optional) Window duration: 4h Keep Rotate immediately when the secret is stored checked In Rotation function: Select the Lambda function: taskhub-backend Click Next. Step 5 – Review \u0026amp; Store the Secret On the Review step, verify the following information:\nSecret type: Other type of secret Encryption key: taskhub_kms Secret name: prod/taskhub/metadata Automatic rotation: Enabled Lambda rotation function: taskhub-backend Scroll down to the Sample code section:\nAWS provides built-in sample getSecret() functions for: Java JavaScript Python C# Go The TaskHub backend will use the corresponding SDK to retrieve secrets from AWS Secrets Manager instead of hard-coding them in source code. Click Store to complete the process.\nResult A new secret has been successfully created in AWS Secrets Manager. The secret content is stored in JSON format. The secret is: Encrypted at rest using KMS (taskhub_kms) Automatically rotatable using AWS Lambda The TaskHub backend can retrieve secrets via: AWS SDK IAM Role / Policy "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/5-workshop/5.18-waf/","title":"Set up Web ACL","tags":[],"description":"","content":"Step 1 – Access AWS WAF \u0026amp; Shield Sign in to the AWS Console Search for the service: WAF \u0026amp; Shield Click AWS WAF\nFrom the left menu, select:\nProtection packs (web ACLs)\nClick the button:\nCreate protection pack (web ACL)\nStep 2 – Select Application Type (Tell us about your app) 2.1 Select App Category Choose:\nAPI \u0026amp; integration services\nStep 3 – Select the CloudFront Distribution to Protect Expand Select resources to protect Click Add resources Select:\nGlobal → Add CloudFront or Amplify resources Check the CloudFront distribution of TaskHub (S3 frontend) Click Add Step 4 – Select the Rule Pack Type Select: ✅ Build your own pack from all of the protections AWS WAF offers\nIn the right panel, select: ✅ AWS-managed rule group\nClick Next Step 5 – Add Amazon IP Reputation List Select the rule: In Rule overrides, configure as follows: Rule Action AWSManagedIPReputationList ✅ Block AWSManagedReconnaissanceList ✅ Block AWSManagedIPDDoSList ✅ Count Click Add rule ✅ After this step, the rule will appear in the Add rules list.\nStep 6 – Verify the Added Rule Verify that the following information appears:\nRule: AWSManagedRulesAmazonIpReputationList Status: Saved WCU: 25 WCU Step 7 – Set the Web ACL Name In the Name and describe section:\nName: taskhub-waf Description: (leave blank or enter any description) Step 8 – Create the Protection Pack (Web ACL) Click: Create protection pack (web ACL)\nWait for AWS to complete the Web ACL creation "},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://chinh140104.github.io/aws_hugo_chinh/tags/","title":"Tags","tags":[],"description":"","content":""}]